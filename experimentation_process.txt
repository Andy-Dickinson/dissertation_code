* When setup grid search, had 18000 configurations, obviously far too many, cut down to 288.
First experiment run (with full dataset (280375 total samples, 196262 training samples) and 288 configurations), in 12 hours 35 mins, only had got to 3rd epoch and 143872/196262 samples of 1st configuration - excessivly long, need to cut down.


* Scaled back training and grid search configurations, cleaned dataset.
Now training to 30 epochs max with early stop patience 5.
Added a timer to the train/validation loops to track as a metric for evaluation.
Added functionality to summarise data, found duplicate rows and rows with only zero descriptor values (won't add value).
Added function to clean data by removing duplicate rows and rows with only zero values, now dataset has 27017 total samples, splitting into train (18911), val (2702), and test (5404) sets.
Grid search configurations cut down to 4 to hopefully allow for quicker training and analysis early on for architectural modifications:
 - batch_sizes = [64]
 - latent_dims = [2, 4]
 - loss_functions = ["mse", "bce"]
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1]  # Ensures KL divergence doesn't overly regularise latent space, which might hinder reconstruction learning
If training is still taking too long, will need to scale back dataset.
 
Visualisations added, ready to visualise the first training configurations.
Later on may want to interpolate the latent space and visualise (extra functionality required).


* Noticed minority descriptor values (1-4) were being heavily predicted in the reconstruction.
Decided to use logarithmic scaling for class weighting to help give the minority classes a boost so the model pays them more attention.
Retrained base configuration with only this change. 

* Reconstructed outputs are much better than before with the log scaling for class weighting, however training time is still too long for the early training stages.
Reduced train and validation datasets to 20% to reduce training time.
Retrainged the base model using the toy dataset using the log scaling class weights to get a toy set baseline for comparison.
Toy model showed similar performance to the base log scaling model (though slightly reduced as expected due to less data).

* Modified model to convolutional model. 
Original FC layers model (base model) had the following:
Encoder: 4 layers (3 trainable) 
	Flatten, 
	FC,  681984 parameters
	FC,  131328 parameters
	Latent FC branching,  514 parameters for each branch assuming latent dim = 2, so 1028 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  768 parameters assuming latent dim = 2
	FC,  131584 parameters
	FC,  682803 parameters
Total: 6 trainable layers,  1629495 parameters

Convolutional model:
Encoder: 4 layers (3 trainable)
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  20736 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters which is fairly close to the original FC model for good comparison

Started training convolutional model.
Base model had a slightly better tradeoff score and loss, whilst the convolution model had a slightly better F1 score,
however all of these are extremely close. 
It should be noted that the best model found in the linear model used a latent dimension of 4, whilst the convolution model used 2. 
The clusters look tighter in the convolution model (good), and there is slight overlapping for both the convolution and linear models (suggesting latent dim is too small).
Due to these being very close, and the clustering being better in the convolution model along with convolution being expected to capture spatial data better than a linear model,
decided to move forward expanding the convolution model.

* Added pooling and upsample layers
Conv_pool model:
Encoder: 6 layers (3 trainable)
	Conv3d,  3584 parameters
	MaxPool3d,  
	Conv3d,  884992 parameters
	MaxPool3d,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (3 trainable)
	FC,  20736 parameters
	Upsample,
	ConvTranspose3d,  884864 parameters
	Upsample,
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters

The model without pooling showed lower reconstruction loss and higher accuracy (more accurately reconstructs the input).
Clusters in the latent space were better separated using UMAP for the model with pooling (may improve latent representations), 
however they are tigher clusters (PCA) for the model without pooling.
It appears the with pooling looses spatial information.

* Removed rounding and clamping operations from VAE. 
Decoder output is now used directly in the loss functions. 
Values are normalised for BCE, but also for other losses to make them comparable in the grid search.
Retraining model without pooling for comparative results going forward.

* Added convolution skip connection in the hope to capture additional features.
Might be worth testing having just a skip connection in the encoder to prevent overfitting.
Conv_skip model:
Encoder: 5 layers (4 trainable)
	(skip) Conv3d,  88064 parameters
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	skip path combined with main path,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 4 layers (4 trainable)
	FC,  20736 parameters
	(skip) ConvTranspose3d,  87809 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 8 trainable layers, 2001158 parameters

Skip connection model pca clusters overlap slightly, UMAP has better seperation, but clusters still overlap slightly.
The convolution model without the skip connection produced better separation in the latent space.
Fluctuations seen in the reconstruction loss and KL divergence.
Model without skip connection had a constent lower loss, though this may be due to overfitting.
The fluctuations seen with the skip connection may be due to the model trying to predict other values.
Though overall the model without the skip connection performed better.

* Trialing deeper network with LeakyReLU to avoid dying relu (usually better for deeper networks)
Also trialing using conv3d in the decoder to try to refine the input before upsampling.
deep model:
Encoder: 6 layers (5 trainable)
	Conv3d,  1792 parameters
	Conv3d,  221312 parameters
	Conv3d,  884992 parameters
	Conv3d,  1769728 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (5 trainable)
	FC,  165888 parameters
	Conv3d,  1769728 parameters
	Conv3d,  884864 parameters
	ConvTranspose3d,  221248 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6142469 parameters

Trialing using the following configurations (4 configurations):
 - batch_sizes = [64]
 - latent_dims = [4, 8]  increasing latent dim
 - loss_functions = ["mse"]  MSE has consistently outperformed BCE.
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1, 0.5]  trialing larger beta value

Demonstrated ability to leverage larger latent space (best model found used latent dim = 8)
Though appears to still be overfitting and biased towards empty space predicting only zeros.
Better reconstruction loss and KL div, though probably because its predicting zeros (simpler outputs).
Potentially may be too heavily regularised by beta 0.1.
Model with best F1 score used beta 0.5, whereas the best performing model used beta 0.1.
Clustering seems more structured latent space distributions, suggesting the architectur does help improve the model.

* Increased training configurations:
latent_dims = [4, 8, 16] 
betas = [0.1, 0.5, 1]

Added batch normalisation in an attempt to stabilise training.

batch_norm model:
Encoder: 10 layers (9 trainable)
	Conv3d,  1792 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  221312 parameters
	BatchNorm3d,  256 parameters
	Conv3d,  884992 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 9 layers (9 trainable)
	FC,  165888 parameters
	BatchNorm1d,  110592 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  884864 parameters
	BatchNorm3d,  256 parameters
	ConvTranspose3d,  221248 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6255365 parameters

Still overfitting and nothing in the reconstructed output. Looking into sparse matrix representations.

* Setup a dataset class. Within that, obtaining sparse representation of data, padded to max number of voxels in the dataset (8 in this case - shape (8,(x,y,z,one-hot descriptors))
Modified model inspired by pointnet (though simpler to see if that works to start with due to small sparse representation matrix (8x8)

Model only predicting a single voxel (though slightly varying its location and descriptor type).
Suspect a few possiblities 
- padded voxels always have coordinates (0,0,0), which make it a safe bet, though currently it doesn't look like the model is predicting only zeros.
- descriptor loss and coordinate loss are not equal scales.
- reconstructed tensor observed to be predicting the same for every voxel, this needs investigating. 

* Redefined model architecture to be more inline with pointnet (1D conv layers and batchnorm).
Major changes made to entire codebase to accommodate. 
Applying transformation matrix to original input coordinates before calculating loss.
Introduced alpha value to balance descriptor loss and coordinate loss. 
Also introduced penalty for duplicate coordinate values, and for incorrect number of padded voxels.
And regularising term for transformation matrix to encourage orthogonality - based on https://medium.com/@itberrios6/point-net-for-classification-968ca64c57a9

Normalsing sampled latent vectors for latent space analysis.

Added penalty and scaling terms to grid search parameters (though using fixed for next run)
Have set alpha to 0.3 (slight more focus on coordinate reconstructions than descriptor values)
dup_pad_penalty_scale to 0.1 (for duplicate/padding penalties - my very quick tests showed that 0.01 may be too small)
lambda_reg to 0.001 (regularising term for transform matrix)

Transformation matrix seems to be overly aggressive and collapsing structure into near 2D and disregarding spatial information and some voxels.
Voxels are getting lost in the transformation process, though the voxel types appear to match what is present.
The reconstructed visualisation further looses voxels and does not always match what was present in the transformed matrix.

* Removed application of transformation to original input in the loss function. Thought is that potential double transformation (in the encoder and in the loss) is 
increasing errors, and potentially allowing the network to learn to cheat by adjusting the transformation matrix.
Changed grid search configurations used (16 configurations):
batch_sizes = [64]
    latent_dims = [8, 16]
    loss_functions = ["mse"]
    optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
    learning_rates = [1e-3]
    weight_decay = [0]
    betas = [0.1, 1]
    alphas = [0.3, 0.5]
    dup_pad_penalty_scales = [0.1, 0.2]
    lambda_regs = [0.001]
	
Transformed input (now only applied for visualisation purposes and not in the loss) looks much closer to the original input, 
it is slightly moving some voxels, but assume this should be expected.

Reconstructed output still only reconstructing a single voxel,
seems to mainly prefer a similar location/coordinate (though it does slightly vary this sometimes, but not much). 
Also seems to prefer blue (wheels), though it does vary this sometimes, and the samples visualised all contain at least 50% blue (wheels).

Looking at the best performing model plots, 
accuracy increases up until around epoch 9, when it seems to flatline (mostly) at around 37%. 
F1 does similar. 
Reconstruction loss (which includes descriptor loss, coordinate loss (scaled by alpha), plus the duplicate penalty and the transform_reg regularising term, 
continually comes down but very very slowly.

* Model may be over regularising, so should maybe trial lower beta values and higher alpha values to increase influence of recon loss and descriptor accuracy.
Not enough voxels are being predicted, so maybe should increase duplicate/padding penalty.
Trialing strong dup penalty, alpha scaling, and lower beta:
batch_sizes = [64]
    latent_dims = [8, 16]
    loss_functions = ["mse"]
    optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
    learning_rates = [1e-3]
    weight_decay = [0]
    betas = [0.05, 0.1]
    alphas = [0.7]
    dup_pad_penalty_scales = [1]
    lambda_regs = [0.001]
	
Model still struggling to reconstruct more than 1 voxel, though descriptor variance has improved.
Descriptor loss steadily decreases, but coordinate loss flatlines.
Latent space still well clustered.

* Scales of each loss and penalties are different, 
e.g. coordinate loss is around 0.14, though desc_loss is between 1.6 and 1.2 (unscaled) (10 times difference) - adjusting alpha
Total loss (which includes the scaled desc_loss and coor_loss, along with the dup penalty and transformation reg) is around 3.3-2.3, whilst scaled kl div is less than 0.08 (non scaled around 1).
Keeping beta low to allow focus on tuning other parameters first (allowing losse latent space).
Trialing intermediate latent_dims, smaller learning rate, and weight decay.
Training for longer as the task is complex, plus the model is completing the runs rather than early stopping without plateauing (150 epochs, with patience 10)

batch_sizes = [64]
latent_dims = [8, 12, 16]
loss_functions = ["mse"] 
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-3, 5e-4]
weight_decay = [0, 1e-4]
betas = [0.01]
alphas = [0.1]
dup_pad_penalty_scales = [1] 
lambda_regs = [0.001]

None of the models trained to completion, most trained to around 30 epochs. 
Unsurprisingly the best model using the tradeoff score, was the only one that ran past 50 (ran to 70 epochs), 
and was the model using latent_dim=12, learning rate=5e-4, weight_decay=0

It managed to reconstruct up to 3 voxels, and varied the type (blue and green) though did not match the original. 
The position was slightly varied, but mostly remained fixed. 

The total loss was mostly flat until around epoch 50 when it came down, 
before rising again around epoch 55. 
Beta scaled KL div constantly increased. 
Coordinate loss and descriptor loss both came down, though not a great deal. 
Dup/pad penalty reduced around epoch 50 and increased around epoch 55 same as the loss.

* Patience increased to 20
Added a fixed scaling to the coordinate loss to bring it into a similar range as descriptor loss (x10)
Trialing another small learning rate, some other small betas, and alphas.
Hoping the coordinate loss will decrease better, and more coordinates explored.

batch_sizes = [64] 
latent_dims = [8, 12, 16]
loss_functions = ["mse"]
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-3, 5e-4, 1e-4]
weight_decay = [0, 1e-5, 1e-4]
betas = [0.01, 0.02, 0.05]
alphas = [0.1, 0.2, 0.3]  
dup_pad_penalty_scales = [1] 
lambda_regs = [0.001]

The best model found (tradeoff), plus the best loss model (which was the same model), used:
latent_dims = [16]
learning_rates = [5e-4] 
weight_decay = [1e-5] 
betas = [0.01]
alphas = [0.2]

Whilst the best F1 model used:
latent_dims = [12] 
learning_rates = [1e-4]
weight_decay = [0]
betas = [0.01]
alphas = [0.3]    

The reconstructed robots all had blue (wheels) only except for one of the samples from the F1 model.
All models also only had around 2 or 3 voxels reconstructed, though it constructed them in various ways, but they tended to remain in a specific corner of the matrix (3D space).

Coordinate loss takes a sharp drop around epoch 10, but then flatlines. 
Descriptor loss took a sharp drop around epoch 5, then drops slowly until around epoch 30, and then flatlines.
Duplicate pad penalty sharply drops until around epoch 20, and then rises again before an oscillating flatline.
Reconstruction loss (which includes all of the above losses and penalties drops, and then flatlines in a similar way.
Beta scaled KL divergence steadily increases.
F1 steadily increases until around epoch 40 before flatlining.
Accuracy does similar.

Realised that in the plots coordinate loss is not the fixed scaled (adjusted) coordinate loss, 
but the raw loss and so it is still shown in a low range (around 0.1-0.14) compared to dup pad (around 0.6-1.2) which is scaled in the plots. 
desc_loss (1.2-1.6) is the raw values, but that doesn't have a scaling.
Adjusted this in the code, so future plots will show the adjusted losses.
Not showing alpha scaled however.
