* When setup grid search, had 18000 configurations, obviously far too many, cut down to 288.
First experiment run (with full dataset (280375 total samples, 196262 training samples) and 288 configurations), in 12 hours 35 mins, only had got to 3rd epoch and 143872/196262 samples of 1st configuration - excessivly long, need to cut down.


* Scaled back training and grid search configurations, cleaned dataset.
Now training to 30 epochs max with early stop patience 5.
Added a timer to the train/validation loops to track as a metric for evaluation.
Added functionality to summarise data, found duplicate rows and rows with only zero descriptor values (won't add value).
Added function to clean data by removing duplicate rows and rows with only zero values, now dataset has 27017 total samples, splitting into train (18911), val (2702), and test (5404) sets.
Grid search configurations cut down to 4 to hopefully allow for quicker training and analysis early on for architectural modifications:
 - batch_sizes = [64]
 - latent_dims = [2, 4]
 - loss_functions = ["mse", "bce"]
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1]  # Ensures KL divergence doesn't overly regularise latent space, which might hinder reconstruction learning
If training is still taking too long, will need to scale back dataset.
 
Visualisations added, ready to visualise the first training configurations.
Later on may want to interpolate the latent space and visualise (extra functionality required).


* Noticed minority descriptor values (1-4) were being heavily predicted in the reconstruction.
Decided to use logarithmic scaling for class weighting to help give the minority classes a boost so the model pays them more attention.
Retrained base configuration with only this change. 

* Reconstructed outputs are much better than before with the log scaling for class weighting, however training time is still too long for the early training stages.
Reduced train and validation datasets to 20% to reduce training time.
Retrainged the base model using the toy dataset using the log scaling class weights to get a toy set baseline for comparison.
Toy model showed similar performance to the base log scaling model (though slightly reduced as expected due to less data).

* Modified model to convolutional model. 
Original FC layers model (base model) had the following:
Encoder: 4 layers (3 trainable) 
	Flatten, 
	FC,  681984 parameters
	FC,  131328 parameters
	Latent FC branching,  514 parameters for each branch assuming latent dim = 2, so 1028 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  768 parameters assuming latent dim = 2
	FC,  131584 parameters
	FC,  682803 parameters
Total: 6 trainable layers,  1629495 parameters

Convolutional model:
Encoder: 4 layers (3 trainable)
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  20736 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters which is fairly close to the original FC model for good comparison

Started training convolutional model.
Base model had a slightly better tradeoff score and loss, whilst the convolution model had a slightly better F1 score,
however all of these are extremely close. 
It should be noted that the best model found in the linear model used a latent dimension of 4, whilst the convolution model used 2. 
The clusters look tighter in the convolution model (good), and there is slight overlapping for both the convolution and linear models (suggesting latent dim is too small).
Due to these being very close, and the clustering being better in the convolution model along with convolution being expected to capture spatial data better than a linear model,
decided to move forward expanding the convolution model.

* Added pooling and upsample layers
Conv_pool model:
Encoder: 6 layers (3 trainable)
	Conv3d,  3584 parameters
	MaxPool3d,  
	Conv3d,  884992 parameters
	MaxPool3d,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (3 trainable)
	FC,  20736 parameters
	Upsample,
	ConvTranspose3d,  884864 parameters
	Upsample,
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters

The model without pooling showed lower reconstruction loss and higher accuracy (more accurately reconstructs the input).
Clusters in the latent space were better separated using UMAP for the model with pooling (may improve latent representations), 
however they are tigher clusters (PCA) for the model without pooling.
It appears the with pooling looses spatial information.

* Removed rounding and clamping operations from VAE. 
Decoder output is now used directly in the loss functions. 
Values are normalised for BCE, but also for other losses to make them comparable in the grid search.
Retraining model without pooling for comparative results going forward.

* Added convolution skip connection in the hope to capture additional features.
Might be worth testing having just a skip connection in the encoder to prevent overfitting.
Conv_skip model:
Encoder: 5 layers (4 trainable)
	(skip) Conv3d,  88064 parameters
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	skip path combined with main path,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 4 layers (4 trainable)
	FC,  20736 parameters
	(skip) ConvTranspose3d,  87809 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 8 trainable layers, 2001158 parameters

Skip connection model pca clusters overlap slightly, UMAP has better seperation, but clusters still overlap slightly.
The convolution model without the skip connection produced better separation in the latent space.
Fluctuations seen in the reconstruction loss and KL divergence.
Model without skip connection had a constent lower loss, though this may be due to overfitting.
The fluctuations seen with the skip connection may be due to the model trying to predict other values.
Though overall the model without the skip connection performed better.

* Trialing deeper network with LeakyReLU to avoid dying relu (usually better for deeper networks)
Also trialing using conv3d in the decoder to try to refine the input before upsampling.
deep model:
Encoder: 6 layers (5 trainable)
	Conv3d,  1792 parameters
	Conv3d,  221312 parameters
	Conv3d,  884992 parameters
	Conv3d,  1769728 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (5 trainable)
	FC,  165888 parameters
	Conv3d,  1769728 parameters
	Conv3d,  884864 parameters
	ConvTranspose3d,  221248 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6142469 parameters

Trialing using the following configurations (4 configurations):
 - batch_sizes = [64]
 - latent_dims = [4, 8]  increasing latent dim
 - loss_functions = ["mse"]  MSE has consistently outperformed BCE.
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1, 0.5]  trialing larger beta value

Demonstrated ability to leverage larger latent space (best model found used latent dim = 8)
Though appears to still be overfitting and biased towards empty space predicting only zeros.
Better reconstruction loss and KL div, though probably because its predicting zeros (simpler outputs).
Potentially may be too heavily regularised by beta 0.1.
Model with best F1 score used beta 0.5, whereas the best performing model used beta 0.1.
Clustering seems more structured latent space distributions, suggesting the architectur does help improve the model.

* Increased training configurations:
latent_dims = [4, 8, 16] 
betas = [0.1, 0.5, 1]

Added batch normalisation in an attempt to stabilise training.

batch_norm model:
Encoder: 10 layers (9 trainable)
	Conv3d,  1792 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  221312 parameters
	BatchNorm3d,  256 parameters
	Conv3d,  884992 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 9 layers (9 trainable)
	FC,  165888 parameters
	BatchNorm1d,  110592 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  884864 parameters
	BatchNorm3d,  256 parameters
	ConvTranspose3d,  221248 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6255365 parameters

Still overfitting and nothing in the reconstructed output. Looking into sparse matrix representations.

* Setup a dataset class. Within that, obtaining sparse representation of data, padded to max number of voxels in the dataset (8 in this case - shape (8,(x,y,z,one-hot descriptors))
Modified model inspired by pointnet (though simpler to see if that works to start with due to small sparse representation matrix (8x8)

Model only predicting a single voxel (though slightly varying its location and descriptor type).
Suspect a few possiblities 
- padded voxels always have coordinates (0,0,0), which make it a safe bet, though currently it doesn't look like the model is predicting only zeros.
- descriptor loss and coordinate loss are not equal scales.
- reconstructed tensor observed to be predicting the same for every voxel, this needs investigating. 

* Redefined model architecture to be more inline with pointnet (1D conv layers and batchnorm).
Major changes made to entire codebase to accommodate. 
Applying transformation matrix to original input coordinates before calculating loss.
Introduced alpha value to balance descriptor loss and coordinate loss. 
Also introduced penalty for duplicate coordinate values, and for incorrect number of padded voxels.
And regularising term for transformation matrix to encourage orthogonality - based on https://medium.com/@itberrios6/point-net-for-classification-968ca64c57a9

Normalsing sampled latent vectors for latent space analysis.

Added penalty and scaling terms to grid search parameters (though using fixed for next run)
Have set alpha to 0.3 (slight more focus on coordinate reconstructions than descriptor values)
dup_pad_penalty_scale to 0.1 (for duplicate/padding penalties - my very quick tests showed that 0.01 may be too small)
lambda_reg to 0.001 (regularising term for transform matrix)

Transformation matrix seems to be overly aggressive and collapsing structure into near 2D and disregarding spatial information and some voxels.
Voxels are getting lost in the transformation process, though the voxel types appear to match what is present.
The reconstructed visualisation further looses voxels and does not always match what was present in the transformed matrix.

* Removed application of transformation to original input in the loss function. Thought is that potential double transformation (in the encoder and in the loss) is 
increasing errors, and potentially allowing the network to learn to cheat by adjusting the transformation matrix.
Changed grid search configurations used (16 configurations):
batch_sizes = [64]
    latent_dims = [8, 16]
    loss_functions = ["mse"]
    optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
    learning_rates = [1e-3]
    weight_decay = [0]
    betas = [0.1, 1]
    alphas = [0.3, 0.5]
    dup_pad_penalty_scales = [0.1, 0.2]
    lambda_regs = [0.001]
	
Transformed input (now only applied for visualisation purposes and not in the loss) looks much closer to the original input, 
it is slightly moving some voxels, but assume this should be expected.

Reconstructed output still only reconstructing a single voxel,
seems to mainly prefer a similar location/coordinate (though it does slightly vary this sometimes, but not much). 
Also seems to prefer blue (wheels), though it does vary this sometimes, and the samples visualised all contain at least 50% blue (wheels).

Looking at the best performing model plots, 
accuracy increases up until around epoch 9, when it seems to flatline (mostly) at around 37%. 
F1 does similar. 
Reconstruction loss (which includes descriptor loss, coordinate loss (scaled by alpha), plus the duplicate penalty and the transform_reg regularising term, 
continually comes down but very very slowly.

* Model may be over regularising, so should maybe trial lower beta values and higher alpha values to increase influence of recon loss and descriptor accuracy.
Not enough voxels are being predicted, so maybe should increase duplicate/padding penalty.
Trialing strong dup penalty, alpha scaling, and lower beta:
batch_sizes = [64]
    latent_dims = [8, 16]
    loss_functions = ["mse"]
    optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
    learning_rates = [1e-3]
    weight_decay = [0]
    betas = [0.05, 0.1]
    alphas = [0.7]
    dup_pad_penalty_scales = [1]
    lambda_regs = [0.001]
	
Model still struggling to reconstruct more than 1 voxel, though descriptor variance has improved.
Descriptor loss steadily decreases, but coordinate loss flatlines.
Latent space still well clustered.

* Scales of each loss and penalties are different, 
e.g. coordinate loss is around 0.14, though desc_loss is between 1.6 and 1.2 (unscaled) (10 times difference) - adjusting alpha
Total loss (which includes the scaled desc_loss and coor_loss, along with the dup penalty and transformation reg) is around 3.3-2.3, whilst scaled kl div is less than 0.08 (non scaled around 1).
Keeping beta low to allow focus on tuning other parameters first (allowing losse latent space).
Trialing intermediate latent_dims, smaller learning rate, and weight decay.
Training for longer as the task is complex, plus the model is completing the runs rather than early stopping without plateauing (150 epochs, with patience 10)

batch_sizes = [64]
latent_dims = [8, 12, 16]
loss_functions = ["mse"] 
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-3, 5e-4]
weight_decay = [0, 1e-4]
betas = [0.01]
alphas = [0.1]
dup_pad_penalty_scales = [1] 
lambda_regs = [0.001]

None of the models trained to completion, most trained to around 30 epochs. 
Unsurprisingly the best model using the tradeoff score, was the only one that ran past 50 (ran to 70 epochs), 
and was the model using latent_dim=12, learning rate=5e-4, weight_decay=0

It managed to reconstruct up to 3 voxels, and varied the type (blue and green) though did not match the original. 
The position was slightly varied, but mostly remained fixed. 

The total loss was mostly flat until around epoch 50 when it came down, 
before rising again around epoch 55. 
Beta scaled KL div constantly increased. 
Coordinate loss and descriptor loss both came down, though not a great deal. 
Dup/pad penalty reduced around epoch 50 and increased around epoch 55 same as the loss.

* Patience increased to 20
Added a fixed scaling to the coordinate loss to bring it into a similar range as descriptor loss (x10)
Trialing another small learning rate, some other small betas, and alphas.
Hoping the coordinate loss will decrease better, and more coordinates explored.

batch_sizes = [64] 
latent_dims = [8, 12, 16]
loss_functions = ["mse"]
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-3, 5e-4, 1e-4]
weight_decay = [0, 1e-5, 1e-4]
betas = [0.01, 0.02, 0.05]
alphas = [0.1, 0.2, 0.3]  
dup_pad_penalty_scales = [1] 
lambda_regs = [0.001]

The best model found (tradeoff), plus the best loss model (which was the same model), used:
latent_dims = [16]
learning_rates = [5e-4] 
weight_decay = [1e-5] 
betas = [0.01]
alphas = [0.2]

Whilst the best F1 model used:
latent_dims = [12] 
learning_rates = [1e-4]
weight_decay = [0]
betas = [0.01]
alphas = [0.3]    

The reconstructed robots all had blue (wheels) only except for one of the samples from the F1 model.
All models also only had around 2 or 3 voxels reconstructed, though it constructed them in various ways, but they tended to remain in a specific corner of the matrix (3D space).

Coordinate loss takes a sharp drop around epoch 10, but then flatlines. 
Descriptor loss took a sharp drop around epoch 5, then drops slowly until around epoch 30, and then flatlines.
Duplicate pad penalty sharply drops until around epoch 20, and then rises again before an oscillating flatline.
Reconstruction loss (which includes all of the above losses and penalties drops, and then flatlines in a similar way.
Beta scaled KL divergence steadily increases.
F1 steadily increases until around epoch 40 before flatlining.
Accuracy does similar.

Realised that in the plots, coordinate loss is not the fixed scaled (adjusted) coordinate loss, 
but the raw loss and so it is still shown in a low range (around 0.1-0.14) compared to dup pad (around 0.6-1.2) which is scaled in the plots. 
desc_loss (1.2-1.6) is the raw values, but that doesn't have a scaling.
Adjusted this in the code, so future plots will show the adjusted losses.
Not showing alpha scaled however.

PROBLEMS IDENTIFIED:
Inspecting the reconstructed output closer, the coordinates are not duplicates, but are very close. 
Meaning when the sparse grid is converted back to a dense one for visualisations, the coordinates are collapsing to duplicates.
The way the coordinate loss is calculated also assumes a 1-to-1 correspondence, which is not the case due to shuffling.
Further to this, the way the duplicate and pad penalty is calculated and applied is non differentiable and so the model isn't learning directly.
The descriptor loss is also being affected by the shuffling. CrossEntropyLoss is order dependant. The shuffling is still required to avoid a positional bias.

* Changed how padding penalty is calculated and applied so it is now differentiable so the model can learn.
Now taking the smooth L1 (differentiable absolute) difference of padded voxels. Using softmax as this is differentiable.

Added a matching loss, loosely inspired by chamfer distance.
Calculates pairwise distance from reconstructed coordinates to original coordinates (rows represent reconstructed, columns are original).
Masks out padded voxels (based on descriptor designation) to ignore the coordinates in the penalty calculation.
Applies softmin (differentiable) to work out nearest neighbour probability - also scales the distance matrix to encourage points to focus on a single point.
Normalises probabilities across the rows (vertically) to create competition and encourage points to not cluster around the same original point.
Averages for the batch.

Added an overlap penalty, calculated from the pairwise distance between matching coordinates.
The negative sum of the log of the upper triangle of the distance matrix (excluding the diagonal)
is calculated as the penalty and then averaged over the batch.

Added a loss for descriptor values based on nearest neighbour.
Calculates the pairwise distance between original and reconstructed points.
Padded voxels are then masked, before obtaining the most likely descriptor value based on nearest original voxel.
Loss is calculated by taking the cross entropy (model must output raw logits).
This also means descriptor values are tied to location, which hopefully the model will learn.

These losses and penalties replace the original loss functions (which relied on order, and were not applied in a differential manner).

Quick training runs to get a feel for the lambda scaling hyperparameters showed noticable improvements. 
All run with:
batch_sizes = [64] 
latent_dims = [16]
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-5]
lambda_reg = 0.001

- Started with 
lambda_coord and lambda_desc = 1, 
and lambda_pad and lambda_collapse = 0.1, 
using beta = 0.01  
2 epochs
-> tighly clustered voxels, some correctly identified, noticable misplacelemt. Collapse starts high, drops significantly. Coor_loss and Desc_loss descrease slowly (but not much).

- collapse = 0.5, pad = 0.2, 20 epochs
-> layouts more spreadout, still tendency for clustering, and descriptors not matching.
coor_loss steady decrease
desc_loss gradual improvement.
padded_pen slight increase.
collapse sharp decrease, and reduced overlap
beta*kl decreases significantly and stabilises, effective utilisation of latent space.

- coord, desc = 1.5
pad = 0.1
collapse = 0.4
-> More descriptor varitation, padded voxels under predicted (too many non padded). 
Scaled pad penalty increases (in range [0.03,0.05]
Other scaled losses/penalties steadily and slowly decrease (though around 2).
Collapse penalty steeply drops from 8 to near 0 in around 3 epochs
Beta*KL stabilises at low value, could be under utilising the latent space.

- coord, desc = 2 
pad = 0.3 
collapse = 0.2
beta = 0.03
-> variation in descriptors still, but not alot of variation in placement or padded voxels. Layout seems fairly rigid.
Collapse penalty still drops rapidly, penalty already strong enough.

- coord = 3
pad = 0.5
collapse = 0.1
-> Still aren't getting correct number of padded voxels (visualisation with 1 original, 7 reconstructed - could be affected by average in dataset), most of the visualised robots contain approx 7.
Slight variation in descriptor values, but limited.
Collapse penalty higher than others at beginning and drops rapidly (2 to near 0). 
Scaled pad penalty is the lowest (0.12 slowly rising to ~0.15), the other scaled penalties are between start much higher, 
collapse penalty quickly drops from 2 to close to 0, whilst desc and coor losses decrease slowly (coor being slightly higher dropping from 6-4 whilst desc drops from 4-2). 

After making a few lambda scaling adjustements,
descriptor diversity increased, showing better descriptor learning, 
but the model consistently over predicts non-padded voxels, indicating that the padded voxel penalty may be too weak or influenced by competing losses. 
The coordinate layout remains somewhat rigid with limited spatial variation, suggesting that the model struggles to generalise positional flexibility. 
The collapse penalty drops sharply in early epochs, helping prevent voxel overlap, but its rapid decrease suggests the overlap penalty may either be too strong 
or an adaptive scaling is required, but it may be affecting other losses. 
Coordinate and descriptor losses decreased steadily, the padded penalty remained low, implying insufficient regularisation. 
Latent space analysis via PCA and UMAP showed well-separated clusters, reflecting effective latent structuring but possibly contributing to rigid reconstructions. 
Adjusting lambda_coord and lambda_desc improved spatial spread and descriptor variation. 
However, the padded voxel penaltys ineffectiveness and limited layout diversity highlight the need for stronger regularisation and stronger or more dynamic penalty scaling. 
In short:
Descriptor matching improved with higher lambda_desc values.
Collapse penalty scaling effectively reduced overlaps quickly.
Padded Voxel Penalty remains ineffective despite adjustments.
Limited coordinate diversity, still feels fixed. 
Collapse penalty sharply drops from ~8 to near zero within around 3 epochs.

* Started some targeted training runs.
Best loss model:
lambda_coord, lambda_desc = 1.5	
lambda_pad = 0.1
lambda_collapse = 0.4
beta = 0.02

Collapse penalty rapidly drops, but it does start extreamly high (around 80, and then drops to near 0). 
Pad penalty gradually rises and looks like it almost stabilises. 
Coord loss and desc loss both slowly decrease. 
Beta scaled kl increases rapidly around epoch 90. 
All the scaled losses except pad penalty gradually decrease, and are in the range 4-0. 
Scaled pad penalty rises before it looks like it almost stabilises, however the scaled loss is small compared to the others (in the range 0.01-0.02). 
Latent space still looks well segregated. 

Whilst the best loss model shows points moving around (though they seem to be targeted towards the centre), the spatial variance is fairly similar. 
Descriptor variance is higher. But in terms of padded voxels, the number of non-padded voxels appears to be around 4 (4 padded voxels). 
The best f1 model where coordinate position isn't accounted for (using configuration coord=3, desc=1.5, pad=0.1, collapse=0.2, trans=0.001, beta=0.03) 
appears to have more non-padded voxels, but appears to be less flexible in them as it seems they are almost fixed at 7 non-padded voxels.

Collapse penalty may be too aggressive, this may also affect exploration ability in the latent space.
Pad penalty can be seen to be effective, however if its too strong, it may restrict model flexibility.
It may be too weak, leading to over-prediction of non-padded voxels.
Both coord loss and desc loss descrease steadily, but coordinate loss seems to dominate.
Models with high collapse penalty struggle with descriptor diversity.

* Beta and learning rate targeted runs based on best loss model (tuned slightly before new losses).
Training to 100 epochs with patience 15.

Best loss model used:
learning rate = 1e-5
beta = 1.0
Raw values: pad penalty and coordinate loss increase. It also looks like they start to stabilise towards the end. Descriptor loss and collapse penalty seem fairly steady.
Scaled losses/penalties: Desc, Pad, and collapse are fairly steady and low ~0.5, coordinate loss increases, but seems to stabilise towards the end of training at around 6.
Though this model was deemed the best loss, it took a sharp dip at epoch 11 before increasing, so it may be masking how potentially bad this model really is. 
beta scaled KL drops quickly initially and then continues to decline slowly. 
The best loss model seems overly regularised and constrained, producing either no voxels at all, or only a single voxel in the same location (blue - wheels).
The best f1 model used:
learning rate = 3e-05 and beta = 0.1 and produced more voxels, had a better variety of descriptor values, but limited spatial variance.
Models with a lower beta produced better spatial and descriptor variance, however too low could potentially lead to a latent space that isn't meaningful.

* Targeted tuning for

