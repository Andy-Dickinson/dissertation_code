* When setup grid search, had 18000 configurations, obviously far too many, cut down to 288.
First experiment run (with full dataset (280375 total samples, 196262 training samples) and 288 configurations), in 12 hours 35 mins, only had got to 3rd epoch and 143872/196262 samples of 1st configuration - excessivly long, need to cut down.


* Scaled back training and grid search configurations, cleaned dataset.
Now training to 30 epochs max with early stop patience 5.
Added a timer to the train/validation loops to track as a metric for evaluation.
Added functionality to summarise data, found duplicate rows and rows with only zero descriptor values (won't add value).
Added function to clean data by removing duplicate rows and rows with only zero values, now dataset has 27017 total samples, splitting into train (18911), val (2702), and test (5404) sets.
Grid search configurations cut down to 4 to hopefully allow for quicker training and analysis early on for architectural modifications:
 - batch_sizes = [64]
 - latent_dims = [2, 4]
 - loss_functions = ["mse", "bce"]
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1]  # Ensures KL divergence doesn't overly regularise latent space, which might hinder reconstruction learning
If training is still taking too long, will need to scale back dataset.
 
Visualisations added, ready to visualise the first training configurations.
Later on may want to interpolate the latent space and visualise (extra functionality required).


* Noticed minority descriptor values (1-4) were being heavily predicted in the reconstruction.
Decided to use logarithmic scaling for class weighting to help give the minority classes a boost so the model pays them more attention.
Retrained base configuration with only this change. 

* Reconstructed outputs are much better than before with the log scaling for class weighting, however training time is still too long for the early training stages.
Reduced train and validation datasets to 20% to reduce training time.
Retrainged the base model using the toy dataset using the log scaling class weights to get a toy set baseline for comparison.
Toy model showed similar performance to the base log scaling model (though slightly reduced as expected due to less data).

* Modified model to convolutional model. 
Original FC layers model (base model) had the following:
Encoder: 4 layers (3 trainable) 
	Flatten, 
	FC,  681984 parameters
	FC,  131328 parameters
	Latent FC branching,  514 parameters for each branch assuming latent dim = 2, so 1028 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  768 parameters assuming latent dim = 2
	FC,  131584 parameters
	FC,  682803 parameters
Total: 6 trainable layers,  1629495 parameters

Convolutional model:
Encoder: 4 layers (3 trainable)
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  20736 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters which is fairly close to the original FC model for good comparison

Started training convolutional model.
Base model had a slightly better tradeoff score and loss, whilst the convolution model had a slightly better F1 score,
however all of these are extremely close. 
It should be noted that the best model found in the linear model used a latent dimension of 4, whilst the convolution model used 2. 
The clusters look tighter in the convolution model (good), and there is slight overlapping for both the convolution and linear models (suggesting latent dim is too small).
Due to these being very close, and the clustering being better in the convolution model along with convolution being expected to capture spatial data better than a linear model,
decided to move forward expanding the convolution model.

* Added pooling and upsample layers
Conv_pool model:
Encoder: 6 layers (3 trainable)
	Conv3d,  3584 parameters
	MaxPool3d,  
	Conv3d,  884992 parameters
	MaxPool3d,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (3 trainable)
	FC,  20736 parameters
	Upsample,
	ConvTranspose3d,  884864 parameters
	Upsample,
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters



Removed rounding and clamping operations from VAE. 
Decoder output is now used directly in the loss functions. 
Values are normalised for BCE, but also for other losses to make them comparable in the grid search.
Retrain pooling model for comparative results going forward.