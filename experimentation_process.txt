* When setup grid search, had 18000 configurations, obviously far too many, cut down to 288.
First experiment run (with full dataset (280375 total samples, 196262 training samples) and 288 configurations), in 12 hours 35 mins, only had got to 3rd epoch and 143872/196262 samples of 1st configuration - excessivly long, need to cut down.


* Scaled back training and grid search configurations, cleaned dataset.
Now training to 30 epochs max with early stop patience 5.
Added a timer to the train/validation loops to track as a metric for evaluation.
Added functionality to summarise data, found duplicate rows and rows with only zero descriptor values (won't add value).
Added function to clean data by removing duplicate rows and rows with only zero values, now dataset has 27017 total samples, splitting into train (18911), val (2702), and test (5404) sets.
Grid search configurations cut down to 4 to hopefully allow for quicker training and analysis early on for architectural modifications:
 - batch_sizes = [64]
 - latent_dims = [2, 4]
 - loss_functions = ["mse", "bce"]
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1]  # Ensures KL divergence doesn't overly regularise latent space, which might hinder reconstruction learning
If training is still taking too long, will need to scale back dataset.
 
Visualisations added, ready to visualise the first training configurations.
Later on may want to interpolate the latent space and visualise (extra functionality required).


* Noticed minority descriptor values (1-4) were being heavily predicted in the reconstruction.
Decided to use logarithmic scaling for class weighting to help give the minority classes a boost so the model pays them more attention.
Retrained base configuration with only this change. 

* Reconstructed outputs are much better than before with the log scaling for class weighting, however training time is still too long for the early training stages.
Reduced train and validation datasets to 20% to reduce training time.
Retrainged the base model using the toy dataset using the log scaling class weights to get a toy set baseline for comparison.
Toy model showed similar performance to the base log scaling model (though slightly reduced as expected due to less data).

* Modified model to convolutional model. 
Original FC layers model (base model) had the following:
Encoder: 4 layers (3 trainable) 
	Flatten, 
	FC,  681984 parameters
	FC,  131328 parameters
	Latent FC branching,  514 parameters for each branch assuming latent dim = 2, so 1028 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  768 parameters assuming latent dim = 2
	FC,  131584 parameters
	FC,  682803 parameters
Total: 6 trainable layers,  1629495 parameters

Convolutional model:
Encoder: 4 layers (3 trainable)
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  20736 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters which is fairly close to the original FC model for good comparison

Started training convolutional model.
Base model had a slightly better tradeoff score and loss, whilst the convolution model had a slightly better F1 score,
however all of these are extremely close. 
It should be noted that the best model found in the linear model used a latent dimension of 4, whilst the convolution model used 2. 
The clusters look tighter in the convolution model (good), and there is slight overlapping for both the convolution and linear models (suggesting latent dim is too small).
Due to these being very close, and the clustering being better in the convolution model along with convolution being expected to capture spatial data better than a linear model,
decided to move forward expanding the convolution model.

* Added pooling and upsample layers
Conv_pool model:
Encoder: 6 layers (3 trainable)
	Conv3d,  3584 parameters
	MaxPool3d,  
	Conv3d,  884992 parameters
	MaxPool3d,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (3 trainable)
	FC,  20736 parameters
	Upsample,
	ConvTranspose3d,  884864 parameters
	Upsample,
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters

The model without pooling showed lower reconstruction loss and higher accuracy (more accurately reconstructs the input).
Clusters in the latent space were better separated using UMAP for the model with pooling (may improve latent representations), 
however they are tigher clusters (PCA) for the model without pooling.
It appears the with pooling looses spatial information.

* Removed rounding and clamping operations from VAE. 
Decoder output is now used directly in the loss functions. 
Values are normalised for BCE, but also for other losses to make them comparable in the grid search.
Retraining model without pooling for comparative results going forward.

* Added convolution skip connection in the hope to capture additional features.
Might be worth testing having just a skip connection in the encoder to prevent overfitting.
Conv_skip model:
Encoder: 5 layers (4 trainable)
	(skip) Conv3d,  88064 parameters
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	skip path combined with main path,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 4 layers (4 trainable)
	FC,  20736 parameters
	(skip) ConvTranspose3d,  87809 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 8 trainable layers, 2001158 parameters

Skip connection model pca clusters overlap slightly, UMAP has better seperation, but clusters still overlap slightly.
The convolution model without the skip connection produced better separation in the latent space.
Fluctuations seen in the reconstruction loss and KL divergence.
Model without skip connection had a constent lower loss, though this may be due to overfitting.
The fluctuations seen with the skip connection may be due to the model trying to predict other values.
Though overall the model without the skip connection performed better.

* Trialing deeper network with LeakyReLU to avoid dying relu (usually better for deeper networks)
Also trialing using conv3d in the decoder to try to refine the input before upsampling.
deep model:
Encoder: 6 layers (5 trainable)
	Conv3d,  1792 parameters
	Conv3d,  221312 parameters
	Conv3d,  884992 parameters
	Conv3d,  1769728 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (5 trainable)
	FC,  165888 parameters
	Conv3d,  1769728 parameters
	Conv3d,  884864 parameters
	ConvTranspose3d,  221248 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6142469 parameters

Trialing using the following configurations (4 configurations):
 - batch_sizes = [64]
 - latent_dims = [4, 8]  increasing latent dim
 - loss_functions = ["mse"]  MSE has consistently outperformed BCE.
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1, 0.5]  trialing larger beta value

Demonstrated ability to leverage larger latent space (best model found used latent dim = 8)
Though appears to still be overfitting and biased towards empty space predicting only zeros.
Better reconstruction loss and KL div, though probably because its predicting zeros (simpler outputs).
Potentially may be too heavily regularised by beta 0.1.
Model with best F1 score used beta 0.5, whereas the best performing model used beta 0.1.
Clustering seems more structured latent space distributions, suggesting the architectur does help improve the model.

* Increased training configurations:
latent_dims = [4, 8, 16] 
betas = [0.1, 0.5, 1]

Added batch normalisation in an attempt to stabilise training.

batch_norm model:
Encoder: 10 layers (9 trainable)
	Conv3d,  1792 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  221312 parameters
	BatchNorm3d,  256 parameters
	Conv3d,  884992 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 9 layers (9 trainable)
	FC,  165888 parameters
	BatchNorm1d,  110592 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  884864 parameters
	BatchNorm3d,  256 parameters
	ConvTranspose3d,  221248 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6255365 parameters

Still overfitting and nothing in the reconstructed output. Looking into sparse matrix representations.

* Setup a dataset class. Within that, obtaining sparse representation of data, padded to max number of voxels in the dataset (8 in this case - shape (8,(x,y,z,one-hot descriptors))
Modified model inspired by pointnet (though simpler to see if that works to start with due to small sparse representation matrix (8x8)

Model only predicting a single voxel (though slightly varying its location and descriptor type).
Suspect a few possiblities 
- padded voxels always have coordinates (0,0,0), which make it a safe bet, though currently it doesn't look like the model is predicting only zeros.
- descriptor loss and coordinate loss are not equal scales.
- reconstructed tensor observed to be predicting the same for every voxel, this needs investigating. 

* Redefined model architecture to be more inline with pointnet (1D conv layers and batchnorm).
Major changes made to entire codebase to accommodate. 
Applying transformation matrix to original input coordinates before calculating loss.
Introduced alpha value to balance descriptor loss and coordinate loss. 
Also introduced penalty for duplicate coordinate values, and for incorrect number of padded voxels.
And regularising term for transformation matrix to encourage orthogonality - based on https://medium.com/@itberrios6/point-net-for-classification-968ca64c57a9

Normalsing sampled latent vectors for latent space analysis.

Added penalty and scaling terms to grid search parameters (though using fixed for next run)
Have set alpha to 0.3 (slight more focus on coordinate reconstructions than descriptor values)
dup_pad_penalty_scale to 0.1 (for duplicate/padding penalties - my very quick tests showed that 0.01 may be too small)
lambda_reg to 0.001 (regularising term for transform matrix)

Transformation matrix seems to be overly aggressive and collapsing structure into near 2D and disregarding spatial information and some voxels.
Voxels are getting lost in the transformation process, though the voxel types appear to match what is present.
The reconstructed visualisation further looses voxels and does not always match what was present in the transformed matrix.

* Removed application of transformation to original input in the loss function. Thought is that potential double transformation (in the encoder and in the loss) is 
increasing errors, and potentially allowing the network to learn to cheat by adjusting the transformation matrix.
Changed grid search configurations used (16 configurations):
batch_sizes = [64]
    latent_dims = [8, 16]
    loss_functions = ["mse"]
    optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
    learning_rates = [1e-3]
    weight_decay = [0]
    betas = [0.1, 1]
    alphas = [0.3, 0.5]
    dup_pad_penalty_scales = [0.1, 0.2]
    lambda_regs = [0.001]
	
Transformed input (now only applied for visualisation purposes and not in the loss) looks much closer to the original input, 
it is slightly moving some voxels, but assume this should be expected.

Reconstructed output still only reconstructing a single voxel,
seems to mainly prefer a similar location/coordinate (though it does slightly vary this sometimes, but not much). 
Also seems to prefer blue (wheels), though it does vary this sometimes, and the samples visualised all contain at least 50% blue (wheels).

Looking at the best performing model plots, 
accuracy increases up until around epoch 9, when it seems to flatline (mostly) at around 37%. 
F1 does similar. 
Reconstruction loss (which includes descriptor loss, coordinate loss (scaled by alpha), plus the duplicate penalty and the transform_reg regularising term, 
continually comes down but very very slowly.

* Model may be over regularising, so should maybe trial lower beta values and higher alpha values to increase influence of recon loss and descriptor accuracy.
Not enough voxels are being predicted, so maybe should increase duplicate/padding penalty.
Trialing strong dup penalty, alpha scaling, and lower beta:
batch_sizes = [64]
    latent_dims = [8, 16]
    loss_functions = ["mse"]
    optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
    learning_rates = [1e-3]
    weight_decay = [0]
    betas = [0.05, 0.1]
    alphas = [0.7]
    dup_pad_penalty_scales = [1]
    lambda_regs = [0.001]
	
Model still struggling to reconstruct more than 1 voxel, though descriptor variance has improved.
Descriptor loss steadily decreases, but coordinate loss flatlines.
Latent space still well clustered.

* Scales of each loss and penalties are different, 
e.g. coordinate loss is around 0.14, though desc_loss is between 1.6 and 1.2 (unscaled) (10 times difference) - adjusting alpha
Total loss (which includes the scaled desc_loss and coor_loss, along with the dup penalty and transformation reg) is around 3.3-2.3, whilst scaled kl div is less than 0.08 (non scaled around 1).
Keeping beta low to allow focus on tuning other parameters first (allowing losse latent space).
Trialing intermediate latent_dims, smaller learning rate, and weight decay.
Training for longer as the task is complex, plus the model is completing the runs rather than early stopping without plateauing (150 epochs, with patience 10)

batch_sizes = [64]
latent_dims = [8, 12, 16]
loss_functions = ["mse"] 
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-3, 5e-4]
weight_decay = [0, 1e-4]
betas = [0.01]
alphas = [0.1]
dup_pad_penalty_scales = [1] 
lambda_regs = [0.001]

None of the models trained to completion, most trained to around 30 epochs. 
Unsurprisingly the best model using the tradeoff score, was the only one that ran past 50 (ran to 70 epochs), 
and was the model using latent_dim=12, learning rate=5e-4, weight_decay=0

It managed to reconstruct up to 3 voxels, and varied the type (blue and green) though did not match the original. 
The position was slightly varied, but mostly remained fixed. 

The total loss was mostly flat until around epoch 50 when it came down, 
before rising again around epoch 55. 
Beta scaled KL div constantly increased. 
Coordinate loss and descriptor loss both came down, though not a great deal. 
Dup/pad penalty reduced around epoch 50 and increased around epoch 55 same as the loss.

* Patience increased to 20
Added a fixed scaling to the coordinate loss to bring it into a similar range as descriptor loss (x10)
Trialing another small learning rate, some other small betas, and alphas.
Hoping the coordinate loss will decrease better, and more coordinates explored.

batch_sizes = [64] 
latent_dims = [8, 12, 16]
loss_functions = ["mse"]
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-3, 5e-4, 1e-4]
weight_decay = [0, 1e-5, 1e-4]
betas = [0.01, 0.02, 0.05]
alphas = [0.1, 0.2, 0.3]  
dup_pad_penalty_scales = [1] 
lambda_regs = [0.001]

The best model found (tradeoff), plus the best loss model (which was the same model), used:
latent_dims = [16]
learning_rates = [5e-4] 
weight_decay = [1e-5] 
betas = [0.01]
alphas = [0.2]

Whilst the best F1 model used:
latent_dims = [12] 
learning_rates = [1e-4]
weight_decay = [0]
betas = [0.01]
alphas = [0.3]    

The reconstructed robots all had blue (wheels) only except for one of the samples from the F1 model.
All models also only had around 2 or 3 voxels reconstructed, though it constructed them in various ways, but they tended to remain in a specific corner of the matrix (3D space).

Coordinate loss takes a sharp drop around epoch 10, but then flatlines. 
Descriptor loss took a sharp drop around epoch 5, then drops slowly until around epoch 30, and then flatlines.
Duplicate pad penalty sharply drops until around epoch 20, and then rises again before an oscillating flatline.
Reconstruction loss (which includes all of the above losses and penalties drops, and then flatlines in a similar way.
Beta scaled KL divergence steadily increases.
F1 steadily increases until around epoch 40 before flatlining.
Accuracy does similar.

Realised that in the plots, coordinate loss is not the fixed scaled (adjusted) coordinate loss, 
but the raw loss and so it is still shown in a low range (around 0.1-0.14) compared to dup pad (around 0.6-1.2) which is scaled in the plots. 
desc_loss (1.2-1.6) is the raw values, but that doesn't have a scaling.
Adjusted this in the code, so future plots will show the adjusted losses.
Not showing alpha scaled however.

PROBLEMS IDENTIFIED:
Inspecting the reconstructed output closer, the coordinates are not duplicates, but are very close. 
Meaning when the sparse grid is converted back to a dense one for visualisations, the coordinates are collapsing to duplicates.
The way the coordinate loss is calculated also assumes a 1-to-1 correspondence, which is not the case due to shuffling.
Further to this, the way the duplicate and pad penalty is calculated and applied is non differentiable and so the model isn't learning directly.
The descriptor loss is also being affected by the shuffling. CrossEntropyLoss is order dependant. The shuffling is still required to avoid a positional bias.

** Changed how padding penalty is calculated and applied so it is now differentiable so the model can learn.
Now taking the smooth L1 (differentiable absolute) difference of padded voxels. Using softmax as this is differentiable.

Added a coordinate matching loss, loosely inspired by the Hungarian algorithm, but instead use cdist to calulate a distance matrix. 
Calculates pairwise distance from reconstructed coordinates to original coordinates (rows represent reconstructed, columns are original).
Masks out padded voxels (based on descriptor designation) to ignore the coordinates in the penalty calculation.
Applies softmin (differentiable) to work out nearest neighbour probability - also scales the distance matrix to encourage points to focus on a single point.
Normalises probabilities across the rows (vertically) to create competition and encourage points to not cluster around the same original point.
Averages for the batch.

Added an overlap penalty, calculated from the pairwise distance between matching coordinates.
The negative sum of the log of the upper triangle of the distance matrix (excluding the diagonal)
is calculated as the penalty and then averaged over the batch.

Added a loss for descriptor values based on nearest neighbour.
Calculates the pairwise distance between original and reconstructed points.
Padded voxels are then masked, before obtaining the most likely descriptor value based on nearest original voxel.
Loss is calculated by taking the cross entropy (model must output raw logits).
This also means descriptor values are tied to location, which hopefully the model will learn.

These losses and penalties replace the original loss functions (which relied on order, and were not applied in a differential manner).

Quick training runs to get a feel for the lambda scaling hyperparameters showed noticable improvements. 
All run with:
batch_sizes = [64] 
latent_dims = [16]
optimizer = [
	{"type": optim.Adam, "params": {}, "model_name": "adam"}
]
learning_rates = [1e-5]
lambda_reg = 0.001

- Started with 
lambda_coord and lambda_desc = 1, 
and lambda_pad and lambda_collapse = 0.1, 
using beta = 0.01  
2 epochs
-> tighly clustered voxels, some correctly identified, noticable misplacelemt. Collapse starts high, drops significantly. Coor_loss and Desc_loss descrease slowly (but not much).

- collapse = 0.5, pad = 0.2, 20 epochs
-> layouts more spreadout, still tendency for clustering, and descriptors not matching.
coor_loss steady decrease
desc_loss gradual improvement.
padded_pen slight increase.
collapse sharp decrease, and reduced overlap
beta*kl decreases significantly and stabilises, effective utilisation of latent space.

- coord, desc = 1.5
pad = 0.1
collapse = 0.4
-> More descriptor varitation, padded voxels under predicted (too many non padded). 
Scaled pad penalty increases (in range [0.03,0.05]
Other scaled losses/penalties steadily and slowly decrease (though around 2).
Collapse penalty steeply drops from 8 to near 0 in around 3 epochs
Beta*KL stabilises at low value, could be under utilising the latent space.

- coord, desc = 2 
pad = 0.3 
collapse = 0.2
beta = 0.03
-> variation in descriptors still, but not alot of variation in placement or padded voxels. Layout seems fairly rigid.
Collapse penalty still drops rapidly, penalty already strong enough.

- coord = 3
pad = 0.5
collapse = 0.1
-> Still aren't getting correct number of padded voxels (visualisation with 1 original, 7 reconstructed - could be affected by average in dataset), most of the visualised robots contain approx 7.
Slight variation in descriptor values, but limited.
Collapse penalty higher than others at beginning and drops rapidly (2 to near 0). 
Scaled pad penalty is the lowest (0.12 slowly rising to ~0.15), the other scaled penalties are between start much higher, 
collapse penalty quickly drops from 2 to close to 0, whilst desc and coor losses decrease slowly (coor being slightly higher dropping from 6-4 whilst desc drops from 4-2). 

After making a few lambda scaling adjustements,
descriptor diversity increased, showing better descriptor learning, 
but the model consistently over predicts non-padded voxels, indicating that the padded voxel penalty may be too weak or influenced by competing losses. 
The coordinate layout remains somewhat rigid with limited spatial variation, suggesting that the model struggles to generalise positional flexibility. 
The collapse penalty drops sharply in early epochs, helping prevent voxel overlap, but its rapid decrease suggests the overlap penalty may either be too strong 
or an adaptive scaling is required, but it may be affecting other losses. 
Coordinate and descriptor losses decreased steadily, the padded penalty remained low, implying insufficient regularisation. 
Latent space analysis via PCA and UMAP showed well-separated clusters, reflecting effective latent structuring but possibly contributing to rigid reconstructions. 
Adjusting lambda_coord and lambda_desc improved spatial spread and descriptor variation. 
However, the padded voxel penaltys ineffectiveness and limited layout diversity highlight the need for stronger regularisation and stronger or more dynamic penalty scaling. 
In short:
Descriptor matching improved with higher lambda_desc values.
Collapse penalty scaling effectively reduced overlaps quickly.
Padded Voxel Penalty remains ineffective despite adjustments.
Limited coordinate diversity, still feels fixed. 
Collapse penalty sharply drops from ~8 to near zero within around 3 epochs.

* Started some targeted training runs.
Best loss model:
lambda_coord, lambda_desc = 1.5	
lambda_pad = 0.1
lambda_collapse = 0.4
beta = 0.02

Collapse penalty rapidly drops, but it does start extreamly high (around 80, and then drops to near 0). 
Pad penalty gradually rises and looks like it almost stabilises. 
Coord loss and desc loss both slowly decrease. 
Beta scaled kl increases rapidly around epoch 90. 
All the scaled losses except pad penalty gradually decrease, and are in the range 4-0. 
Scaled pad penalty rises before it looks like it almost stabilises, however the scaled loss is small compared to the others (in the range 0.01-0.02). 
Latent space still looks well segregated. 

Whilst the best loss model shows points moving around (though they seem to be targeted towards the centre), the spatial variance is fairly similar. 
Descriptor variance is higher. But in terms of padded voxels, the number of non-padded voxels appears to be around 4 (4 padded voxels). 
The best f1 model where coordinate position isn't accounted for (using configuration coord=3, desc=1.5, pad=0.1, collapse=0.2, trans=0.001, beta=0.03) 
appears to have more non-padded voxels, but appears to be less flexible in them as it seems they are almost fixed at 7 non-padded voxels.

Collapse penalty may be too aggressive, this may also affect exploration ability in the latent space.
Pad penalty can be seen to be effective, however if its too strong, it may restrict model flexibility.
It may be too weak, leading to over-prediction of non-padded voxels.
Both coord loss and desc loss descrease steadily, but coordinate loss seems to dominate.
Models with high collapse penalty struggle with descriptor diversity.

* Beta and learning rate targeted runs based on best loss model (tuned slightly before new losses).
Training to 100 epochs with patience 15.

Best loss model used:
learning rate = 1e-5
beta = 1.0
Raw values: pad penalty and coordinate loss increase. It also looks like they start to stabilise towards the end. Descriptor loss and collapse penalty seem fairly steady.
Scaled losses/penalties: Desc, Pad, and collapse are fairly steady and low ~0.5, coordinate loss increases, but seems to stabilise towards the end of training at around 6.
Though this model was deemed the best loss, it took a sharp dip at epoch 11 before increasing, so it may be masking how potentially bad this model really is. 
beta scaled KL drops quickly initially and then continues to decline slowly. 
The best loss model seems overly regularised and constrained, producing either no voxels at all, or only a single voxel in the same location (blue - wheels).
The best f1 model used:
learning rate = 3e-05 and beta = 0.1 and produced more voxels, had a better variety of descriptor values, but limited spatial variance.
Models with a lower beta produced better spatial and descriptor variance, however too low could potentially lead to a latent space that isn't meaningful.

* Targeted tuning using:
Learning rates = 1e-05 and 3e-05
lam cord = 1.2  and 1.5
lam des = 1.5 and 2.0
lam pad = 0.15
collapse = 0.2 and 0.3
beta = 0.1 and 0.5

Noticed padded voxel penalty had a bug. Where I was taking the absolute between the original and reconstructed and modified it to smooth L1, the order was incorrect.
Corrected losses/penalties.

** Targeted tuning to get a feel for new losses:
beta = 0.01 and 0.1   # Found earlier that this was about the correct range
coord = 1.0 and 2.0
desc = 1.0 and 2.0
pad = 1.0
collapse = 1.0 and 2.0
reg = 0.001
epochs = 50, patience = 10


* Running a longer training run testing configuration variations.
Experiments are relative to previous runs in terms of high and low.
All use:
batch size = 64
latent dim = 16
optimizer = Adam
learning rate = 1e-5
weight decay = 1e-5
lambda_reg = 0.001
Observations (not in any order):

Lower Collapse Penalty + Higher Descriptor Penalty 
coord = 1.0 
desc = 2.0
pad = 1.0
collapse = 0.5
beta = 0.01
Clustered in the centre, fewer reconstructed than original voxels. 
Limited layout variation. Minor descriptor variation. 
Scaled losses all downward trends, except pad penalty which increases, however, it is in a much smaller range than the others (max 0.175) 
whilst the others are around 2 or 3. beta scaled kl peaks at around 0.05, and then drops to 0.03, recon loss is around 5.

Very Low Collapse Penalty with High Beta
coord = 1.0 
desc = 1.0
pad = 1.0
collapse = 0.1
beta = 0.1
Still clustering in the centre, only reconstructing 3 voxels when original has much more. 
All scaled losses drop and stabilise, coor loss is higher than the others at around 2.5 (stabilised), desc loss around 0.8, 
pad around 0.3, and collapse near 0. Beta scaled kl looks like a bell curve, but is very small (0.01 to 0.04) compared to the recon loss (around 3)

Descriptor Focus Only (Coord low, Collapse low)
coord = 0.5 
desc = 3.0
pad = 1.0
collapse = 0.5
beta = 0.01
Still some clustering towards centre of voxel space, but more variation in layout and number of voxels. 
Better matching of descriptor colours. scaled pad penalty rises but is low compared to the others in range (0.01-0.1). 
The other scaled losses drop gradually (except collapse that drops rapidly before stabilising near 0, descr loss is in range (5-2.5), 
coord loss is in range (3-2). beta scaled kl rises rapidly around epoch 45 and stabilises around 0.09, whilst recon loss drops and smooths out around 5.

Balanced Spread: Medium Collapse Penalty + Higher Descriptor
coord = 1.0 
desc = 2.0
pad = 1.0
collapse = 1.5
beta = 0.05
Does not reconstruct any voxels. Scaled losses: collapse drops from 1 to near 0, desc and coord loss drop before rising again, 
desc in range (1.25-2), coord in range (0.75, 1.5), pad penalty rises from 0.02 to 0.1. beta scaled kl rises quickly but small 
value to 0.006 and then gradually drops  to 0.005. recon loss drops  from 4.5 to 2 before rising to 3.

Low Collapse + High Pad Penalty
coord = 1.0 
desc = 1.0
pad = 2.0
collapse = 0.2
beta = 0.01
Reconstructs only one voxel near the centre of the descriptor space or nothing at all.
Some variation in descriptor colour. Scaled losses: collapse drops rapidly to near 0, pad penalty is flat around 0.3, 
desc drops very slowly from around 1.5 to 0.8, coor fluctuates slightly and is flat initially around 3, 
takes a dip to 1, and then climbs to 3. Beta scaled kl climbs from 0.0005 to 0.0025, recon loss fluctuates in a similar way to 
scalled coor but is in the range 10-2, so much higher than beta scaled kl.

Higher Coordinate Penalty with High Beta
coord = 3.0 
desc = 1.0
pad = 1.0
collapse = 1.0
beta = 0.1
Only reconstructs 2 green voxels with some minor variation on location, but mainly towards the centre of the descriptor space. 
Scaled coor loss drops and stabilises from around 10 to 7, whilst the other scaled losses are fairly flat, desc loss is around 1, 
whilst pad penalty and collapse penalty is close to 0. KL divergence oscillates forming 2 mountain like sharp peaks, 
ranging from 0.01 to 0.05, whilst recon loss drops from 14 and flatlines around 8.

Medium Collapse + High KL Beta
coord = 1.0 
desc = 1.0
pad = 1.0
collapse = 1.0
beta = 0.2
Clustering near the centre of the descriptor space, with limited layout variation, high number of voxels (seems to be always around the max =8), 
descriptor colour seems to vary ok. Scaled losses: pad penalty rises slightly and is almost flat in the range 0.005-0.006, 
collapse penalty drops quickly from 17 to near 0 in around 20 epochs, coor loss is almost flat around 5, desc is flat around 2. 
Beta scaled KL starts around 0.01 and rises to a sharp peak at 0.015 at epoch 12 before coming down t o0.0025 and slowly rising, recon 
loss drops quickly (by epoch 12) from 25 to around 8 and is then almost flat. 

High Collapse Penalty with Lower Pad Penalty
coord = 1.0 
desc = 1.0
pad = 0.5
collapse = 3.0
beta = 0.01
Still getting clustering near the centre, but more spread than the others so far, slight variation on layout, some variation on descriptor colours. 
Seem to be getting max number of voxels. Scaled losses: pad penalty is almost flat around 0.0025, collapse penalty drops quickly from 30 to near 0 
in around 20 epochs, coor loss is near flat around 5, and desc loss is flat around 2. Beta scaled kl continues to increase from 0 to around 0.035, 
recon loss drops in around 20 epochs from 40 to 6.

Higher Beta and Descriptor Penalty
coord = 1.0 
desc = 2.5
pad = 1.0
collapse = 1.0
beta = 0.15
Clustering with some variation on layout, more variation on descriptor colours, seems to reconstruct maximum number of voxels. 
Scaled losses: pad penalty is almost flat around 0.007, collapse penalty drops fairly quickly from around 3 to near 0, coor loss 
drops gradually from around 5.5 to 4.5, descriptor loss is near flat around 4. Beta scaled KL divergence fluctuates, but its 
overall trend is that it drops from 0.008 to 0.003 and then rises again to 0.006, whilst recon loss drops faster until around 
epoch 12 and then slower from the value 13 to 8.

High Coord + Descriptor + Collapse Together
coord = 2.5 
desc = 2.5
pad = 1.0
collapse = 2.5
beta = 0.01
Clustering with some variation on layout, limited to no descriptor colour variation, seems to reconstruct maximum number of voxels.
Scaled losses: pad penalty rises from 0.01 to 0.04, collapse penalty drops rapidly from 8 to near 0, coor loss drops slightly from 14 to 10, 
desc loss is almost flat around 4. Beta scaled KL rises steeply at around epoch 20 from near 0 to around 0.150, where it slowly increases to 0.175,
whilst recon loss drops slowing over the epochs from 25 to around 14.

Extreme High Beta (0.5)
coord = 1.0 
desc = 1.0
pad = 1.0
collapse = 1.0
beta = 0.5
Either fails to reconstruct anything, or only reconstructs a single green voxel located near the centre with very minimal location variance. 
Scaled losses: pad penalty rises from 0.008 to 0.011, the other penalties drop gradually at around the same rate, collapse penalty from 0.4 to 
near 0, desc loss from 1 to 0.6, coor loss from 2.2 to 1.5. Beta scaled kl drops quickly from 0.012 to 0.003 and then rises to 0.009, 
whilst recon loss drops gradually from 3.5 to 2.25.

Low Beta (0.001) + High Collapse
coord = 1.0 
desc = 1.0
pad = 1.0
collapse = 5.0
beta = 0.001
Some minor clustering, over reconstructed number of voxels, minor variation in layout, almost no variation in descriptor colours. 
Scaled losses: pad penalty rises quickly from 0.006 to 0.009 and then flatlines around 0.008, collapse penalty drops quickly from 35 to near 0, 
coor loss is almost flat around 5, and desc loss is flat around 2. Beta scaled KL rises exponentially from 0 to 0.002 over 80 epochs, 
whilst recon loss almost mirrors this dropping exponentially from 40 to around 6.

Full baseline:
coord = 1.0 
desc = 1.0
pad = 1.0
collapse = 1.0
beta = 0.01
Gets some clustering, some  variation in layout but compressed compared to original, limited to no descriptor colour variation, 
seems to be reconstructing maximum voxels. Scaled losses: pad penalty is almost flat around 0.009, collapse penalty drops rapidly 
from 20 to near 0 in around 20 epochs, coor loss is near flat dropping from around 5 to 4, desc loss is flat around 2. 
Beta scaled kl is increases at an almost constant rate from 0 to 0.09 in 80 epochs, whilst recon loss drops from 27 to 5 in about 15-20 
epochs before almost flatlining. 

Descriptor Heavy + High Pad Penalty
coord = 0.75
desc = 3.0
pad = 2.0
collapse = 1.0
beta = 0.01
Better variation in layout, limited descriptor colour and voxel number variation (only around 2 or 3 reconstructed). 
Scaled losses: pad penalty increases almost linearly from 0.02 to 0.35, collapse penalty drops rapidly from 25 to near 0 in around 
30 epochs, coor loss drops very very slowly from around 4 to 2, desc loss also drops very very slowly from around 5.5 to 2. 
Beta scaled kl increases from around 0 to 0.12, whilst recon loss drops quickly from around 35 to around 10 in approx 10 epochs and 
then continues to drop to around 4 over the total 80 epochs.

Collapse + Pad Only (Minimal Descriptor Influence)
coord = 0.5 
desc = 0.5
pad = 1.5
collapse = 4.0
beta = 0.01
Better separation, but limited layout variation and only 2 voxels reconstructed even with most around 8 voxels in original. No descriptor colour variation.
Scaled losses: Collapse penalty drops rapidly from 15 to near 0 in around 10 epochs, all other losses are fairly flat, coor loss is around 2, desc loss is around 1,
and pad penalty is near 0. Beta scaled kl rises from 0.0006 to a peak at 0.0015 before dropping back down and almost flatlining at 0.0006, whilst recon loss drops 
quickly from 18 to 2.5 and almost flatlining. 

It appears that high collapse penalty reduces clustering but limits layout variation.
High beta limits reconstruction.
When descriptor penalties are prioritised, layout variation is better.
Pad penalty seems to need to be higher than collapse penalty.
Pad penalty is also tied to coor loss due to the way the loss functions are setup. 
Desc loss and coor loss are also going to be linked due to desc loss using a distance function to determine the most likely voxel it should be matching in 
terms of descriptor colour.

* Setup for high pad penalty experiments

Pad penalty above collapse
coord = 2.5
desc = 3.0
pad = 4.0
collapse = 1.0
beta = 0.01
Some variation in spacing between voxels, but seems to follow a fairly fixed layout. Only 4 voxels seems to be produced, but there is good variation in descriptor
colours. Scaled losses: Pad penalty rises from near 0 to 0.5 where it looks like its starting to stabilise, collapse penalty drops rapidly from 80 to near 0 within 
around 5 epochs, coor loss decreases very slowly almost flat at about 10, desc loss is practically flat at around 3. Beta scaled KL rises to 0.15 and then slowly decreases to 0.11 
while recon loss decrease fairly rapidly from 100 to around 20 within about 5 epochs and then continues to decrease slowly to around 8. 

Balanced collapse and lower descriptor with lower pad
coord = 2.0
desc = 2.5
pad = 3.5
collapse = 2.0
beta = 0.005
Similar configuration to pad penalty above collapse but with less variation in descriptor colours, again only 4 voxels seems to be produced. 
Scaled losses: collapse penalty drops rapidly from 25 to near 0 in 10-15 epochs, pad penalty is flat near 0, desc loss slowly decreases from around 4 to 3,
coor loss decreases slowly from 10 to 5. Beta scaled KL starts increasing around epoch 20 and climbs to 0.12, whilst recon loss drops from 40 to around 10 in about
20 epochs where it seems to stabilise. 

Lower descriptor, high pad
coord = 2.0
desc = 2.0
pad = 3.5
collapse = 2.5
beta = 0.015
Limited variation in layout, points cluster slightly, 8 voxels reconstructed even when there are less original, some but limited variation in descriptor colours.
Scaled losses: pad penalty rises from 0.03 to 0.05 in 10 epochs before decreasing to 0.04, collapse penalty drops rapidly from 70 to near 0 in around 20 epochs, coor loss
is flat at 10, desc loss is flat at 4. Scaled beta kl rises continually from 0 to 0.035, while recon loss decreases rapidly from 80 to around 12 in around 20 epochs where it seems to 
stabilise.

Strong descriptor
coord = 3.0
desc = 5.0
pad = 4.5
collapse = 1.0
beta = 0.02
1 voxel reconstructed mostly, sometimes 2 stacked, but no variation in layout or descriptor colour. Scaled losses: pad penalty rises linearly from 0 to 1, collapse penalty
decreases steadily from 1 to near 0, coor and desc losses look like genetle waves, coor is around 6-8, desc is around 5-3. Beta scaled kl increases constantly from 0.001 to 0.005,
whilst recon loss is a gentle wave in the range 11-13.

Reduced collapse for higher voxel count
coord = 2.0
desc = 3.5
pad = 4.0
collapse = 1.5
beta = 0.01
Maximum number of voxels constantly produced, limited variation in layout, hardly any variation in descriptor colour. Scaled losses: pad penalty rises constantly from 0.02 to 0.035,
collapse penalty drops rapidly from 17 to near 0 in around 20 epochs, desc loss is near flat at 5, coor loss slowly decreases from 11 to 8. beta scaled kl increases constantatly from 
epoch 15 from 0 to 0.2, recon loss decreses rapidly from 30 to 15 in around 10 epochs and then very sloly continues to decrease to about 14.

Descriptor focused 
coord = 1.5
desc = 4.0
pad = 3.5
collapse = 2.0
beta = 0.02
More separation between voxels, though only 2 voxels reconstructed, limited variation in descriptor colours or location. scaled losses: pad penalty rises from near 0 to 1, collapse
penalty drops rapidly from 11 to near 0 in about 20 epochs, coor loss drops steadily from 8 to 4, desc loss also drops steadily from 6 to 3. Beta scaled kl rises from near 0 to 0.0075, 
before dropping to 0.4 and then steeply climbing around epoch 70 to 0.0175 and then dropping back down to 0.013, while recon loss drops from 25 to 8 in about 20 epochs and then slowly
decreases to about 7. 

Lower collapse and descriptor
coord = 2.0
desc = 2.0
pad = 3.5
collapse = 1.5
beta = 0.01
Only 2 voxels reconstructed, but good variation on location and descriptor colour. Scaled losses: collapse penalty rapidly drops from 19 to near 0 in around 10 epochs, 
pad penalty is near 0, desc loss is near flat around 2.5 to 2 maybe a slight decrease, coor loss drops steadily from 10 to 4. beta scaled kl climbs to 0.05 at epoch 30 and then fluctuates
between 0.35 and 0.07, while loss decreases quickly from 30 to around 10 in 10 epochs and then slowly drops to 6.

Higher pad penalty to force voxel reconstruction
coord = 2.0
desc = 2.5
pad = 4.5
collapse = 2.0
beta = 0.015
Only 3 blue voxels reconstructed, some limited variation in location, mainly separation between the voxels. scaled losses: pad penalty increases steadily from 0.02 to 0.11, collapse penalty
drops rapidly from 33 to near 0 in about 30 epochs, desc loss is flat around 2.5, coor loss decreases slowly from 12 to 5. Beta scaled kl rises from 0 to 0.18 with some minor fluctuation, 
recon loss drops from 50 to 20 in about 20 epochs and then slowly drops to around 9. 

Strong descriptor with balanced penalties
coord = 3.0
desc = 4.0
pad = 3.5
collapse = 1.0
beta = 0.008
only 2 or 3 voxels reconstructed, limited variation in both location and colour. scaled losses: collapse penalty drops rapidly from 20 to near 0 in around 10 epochs, pad penalty is near flat
with a very slight increase from 0 to 1, desc loss and coor loss both decreas to around epoch 20 when the stabilise, coor loss drops from 15 to 8, and desc loss drops from 6 to 
3. Beta scaled kl increases from 0 to 0.06 between epoch 7 and 25, then it oscillates and continues to climb to 0.07, recon loss decreases logarithmically from 40 to 11. 

Balanced collapse and pad, reduced coordinate
coord = 2.5
desc = 3.0
pad = 4.0
collapse = 1.5
beta = 0.012
Always 7 voxels reconstructed, no variation in descriptor colour, very slight variation in location. Scaled losses: pad penalty rises from 0.025 to 0.035, collapse penalty
drops rapidly from 17 to near 0 logarithmically, desc loss is near flat around 5, coor loss is mostly flat around 12-10. beta scaled kl increases constantly with slight 
fluctuations from 0 to 0.06, recon loss drops logarithmically from 34 to 15. 

Lower descriptor penalty 
coord = 1.8
desc = 2.5
pad = 3.8
collapse = 2.5
beta = 0.009
always 7 voxels reconstructed, seem fairly tightly clustered together, some variation in layout, no variation in colour. scaled losses: pad penalty is flat around 0.04 until 
epoch 40 when it starts to increase to 0.1, collapse penalty drops rapidly from 70 to near 0 in around 20 epochs, coor loss is near flat around 9, desc loss is near flat around 4.
beta scaled kl increases steadily from 0 to 0.11, recon loss drops rapidly from 85 to 20 in about 10 epochs and then continues to slowly drop to around 10.

Descriptor heavy with high pad
coord = 2.0
desc = 3.5
pad = 4.5
collapse = 1.5
beta = 0.02
maximum voxels always reconstructed, seems to cluster, limited variation in location and colour. scaled losses: pad penalty is fairly flat in the range 0.023 to 0.027, collapse penalty
drops rapidly from 10 to near 0 in around 25 epochs, desc loss is flat at 5.5, coor loss decreases steadily from 11 to 8. beta scaled kl rises from near 0 at epoch 20 to 0.2, recon 
loss drops quickly from 27 to 18 by epoch 5, and then slowly decreases to 14.

Low coordinate penalty for voxel spread
coord = 1.5
desc = 3.0
pad = 4.0
collapse = 2.0
beta = 0.006
7 or 8 voxels reconstructed, fairly clustered, limited variation in location, some variation in colour. scaled losses: pad penalty rises from 0.02 to 0.025 in 10 epochs 
where it then seems flat, collapse penalty decreases logarithmically from 35 to near 0, coor loss is flat at 7, desc loss is flat at 5. beta scaled kl rises exponentially 
from 0 to 0.045, recon decreases logarithmically from 45 to 12.

High pad, low collapse, minimal coordinate
coord = 1.5
desc = 2.0
pad = 4.5
collapse = 1.0
beta = 0.01
only 4 voxels reconstructed, some variation in location and colour. scaled losses: collapse penalty drops rapidly from 60 to near 0 in around 10 epochs, pad penalty is flat near 0,
desc loss is flat at 3, coor loss is mostly flat dropping from 8 to 5. beta scaled kl rises from 0 to 0.04 in 30 epochs where it then oscillates, recon loss drops rapidly from 70
to 8 in 10 epochs where it is then flat. 

High coord, desc, and pad with increased beta
coord = 3.5
desc = 3.5
pad = 4.0
collapse = 2.5
beta = 0.02
Over reconstructing voxels, clustering, some variation in location and colour. scaled losses: pad penalty rises continually from 0.02 to 0.055, collapse penalty drops rapidly from
17 to near 0 in around 10 epochs, desc loss is farily flat around 5, coor loss drops steadily from 19 to 13. beta scaled kl rises continually from 0 to 0.27 in 30 epochs where 
it is then fairly flat, recon loss drops quickly from 40 to 25 in about 10 epochs and then slowly drops to 19.

Loss is always much higher than beta scaled kl, in completly different ranges.
Pad penalty seems to be in a much lower scale than the other losses.

* Pad beta targeted runs

Beta scale focused runs:
coord = 2.0
desc = 2.5
pad = 3.5
collapse = 1.5 
beta = 0.5
Only 3 voxels reconstructed (too many padded) constantly in the exact same positions, no layout variation, some descriptor colour variation.

coord = 1.0
desc = 1.25
pad = 1.75
collapse = 0.75 
beta = 0.5
7 or 8 voxels reconstructed (mostly too few padded),  

coord = 2.0
desc = 2.5
pad = 3.5
collapse = 1.5
beta = 1.0

coord = 1.0
desc = 1.25
pad = 1.75
collapse = 0.75
beta = 1.0

coord = 2.0
desc = 2.5
pad = 3.5
collapse = 1.5
beta = 2.0

coord = 1.0
desc = 1.25
pad = 1.75
collapse = 0.75
beta = 2.0


Pad focus runs
coord = 2.0
desc = 2.5
pad = 4.0
collapse = 1.5
beta = 0.5

coord = 2.0
desc = 2.5
pad = 5.0
collapse = 1.5
beta = 0.5

coord = 2.0
desc = 2.5
pad = 6.0
collapse = 1.5
beta = 0.5

coord = 2.0
desc = 2.5
pad = 8.0
collapse = 1.5
beta = 0.5

coord = 2.0
desc = 2.5
pad = 4.0
collapse = 1.5
beta = 1.0

coord = 2.0
desc = 2.5
pad = 5.0
collapse = 1.5
beta = 1.0

coord = 2.0
desc = 2.5
pad = 6.0
collapse = 1.5
beta = 1.0

coord = 2.0
desc = 2.5
pad = 8.0
collapse = 1.5
beta = 1.0

Targed based on previous best performing and observations
coord = 2.5
desc = 3.0
pad = 4.0
collapse = 1.5
beta = 0.5

coord = 2.0
desc = 2.5
pad = 3.5
collapse = 2.0
beta = 0.5

coord = 2.5
desc = 3.0
pad = 4.0
collapse = 1.5
beta = 1.0

coord = 2.0
desc = 2.5
pad = 3.5
collapse = 2.0
beta = 1.0

coord = 2.5
desc = 3.0
pad = 4.0
collapse = 1.5
beta = 2.0

coord = 2.0
desc = 2.5
pad = 3.5
collapse = 2.0
beta = 2.0

coord = 2.0
desc = 2.5
pad = 6.0
collapse = 1.5
beta = 0.5

coord = 2.0
desc = 2.5
pad = 8.0
collapse = 1.5
beta = 0.5


Reconstructions still clustering with limited variation in both location and colour.
Believe there is some bias in the custom losses. 
Looking at the losses closer, suspect it will be better to do away with masking for the coordinate loss by
encoding padded voxels as (11,11,11) rather than (0,0,0) - a genuine location.
Doing this and using entropy should encourage both the correct number of voxels and the correct locations (max entropy is uniform distribution).
This should also mean that the padded voxel penalty can be removed, simplifying the problem.
Removed masking from the descriptor penalty to include padded voxels in the calculations.

** Loss functions updated to treat padded voxels as a genuine coordinate (as above).
Quick run exploration runs:

Baseline Comparison:
coord = 2.0, desc = 2.0, collapse = 2.0, beta = 0.01
Limited spatial and colour variation, clustering near centre. 
Scaled losses: collapse penalty starts much higher than the other losses at 150 (80 not scaled), and drops to near 0 in around 20 epochs, 
desc loss rises slightly and then drops between 3.5 and 4 (1.5 and 2 not scaled), coor loss is logarithmic and drops from 6.5 to 3 (3.25 to 1.5 not scaled).
Beta scaled KL rises from 0 to 0.15 where it stabilises mirroring total loss, though total loss is in the range 175-0.

Test entropys effect:
coord = 4.0, desc = 1.5, collapse = 2.0, beta = 0.005
Voxels seem to be stacked vertically, slight variation in voxel numbers, and slight variation in colour.
Scaled losses: collapse penalty drops from 20 to near 0 (10-0 raw), desc loss is flat around 2.5 (1.8 raw), coor loss decreases (faster at the start) from 8-5 (2-1 raw).
Beta scaled KL rises continually from near 0 to 0.02, total loss drops from 30 to 7 faster near the start. 

Test descriptor matching:
coord = 1.5, desc = 4.0, collapse = 2.0, beta = 0.008
Location variation is moderate, seems to try to be following the original, but in a limited way, colours are ok too, but not perfect. 
Scaled losses: collapse penalty drops from 37 to near 0 in a logarithmic way (18-0 raw), desc loss is flat at 7 (1.5 raw), coor loss is mostly flat with a slight drop from
4-2 (2.5 to 1 raw). Beta scaled KL almost mirrors total loss (in shape not scale) by rising from near 0 to 0.0175, total loss drops logarithmically from 48 to 9.

Test KL Impact:
coord = 2.0, desc = 2.0, collapse = 2.0, beta = 0.05
Some variation in spatial locations that kind of follows the original very lossely, slightly too clustered towards the centre, some colour variation and slight matching.
Scaled losses: collapse penalty drops rapidly from 20-near 0 in about 15 epochs (10-0 raw), desc loss is flat around 3.5 (1.8 raw), coor loss drops slighly at the beginning
and then stabilises from 5-2.5 (2.5-1 raw). Beta scaled KL almost mirrors total loss (in shape), rising from near 0 to 0.08, total loss drops from 30 to 5.

Low Collapse Penalty:
coord = 2.5, desc = 2.5, collapse = 0.5, beta = 0.01
Voxel number vary, but location not a lot, nor do colours vary. Scaled losses: collapse penalty drops quickly from 7.5 to near 0 in around 20 epochs (15-0 raw), 
coor loss drops slowly from 5.5 to 2.8 (2-1 raw), desc loss is mostly flat around 4.5 (2 raw). Beta scaled KL rises continually from 0 to 0.065 and then dropping slightly to 0.055
with some flucuations, total loss drops quickly from 17 to 7.5 where it seems to stabilise. 


Collapse penalty dominates early, desc loss relatively flat so poor learning.
Beta would probably benefit from being increased to emphasise latent space now coordinate is entropy based. 

* Beta models:
Baseline Higher Beta Check
coord = 2.5
desc = 2.5
collapse = 2.0
beta = 0.1
All but 1 voxels line up in a horizontal contiguous row. The colours vary very slightly and 
the other voxel changes position by 1. Scaled losses: collapse penalty drops from 130 to about 10 
(2.5 to 2 raw) logarithmically where it seems to stabilise, desc loss rises from 4 to 5 and then 
starts to decrease back towards 4 (1.75 - 2 raw), coor loss drops steadily logarithmically from
8 to 5 (3.25-1.75 raw). Beta scaled KL rises to a peak at 0.225 after 15 epochs and steadily declines 
to 0.1 forming a mountain shape, total loss drops logarithmically from 150 to 20.

Spatially Focused (High Coordinate, Moderate Descriptor)
coord = 5.0
desc = 2.0
collapse = 1.5
beta = 0.15
Number of voxels reconstructed is moderately good, limited spatial variation and almost no colour
variation. Scaled losses: collapse penalty drops logarithmically from 14 to 0.5 (9-0.5 raw), coor
loss drops smoothly from 10 to 6 (2-1 raw) where it seems to stabilise, desc loss is almost flat
at 3.5 (2 raw). Beta scaled kl looks like 2 mountain peaks, initially rising from 0.01 to the first
peak at 0.05, then it drops to 0.02 then rises to the second peak at 0.055 and then drops again to 0.01,
total loss drops logarithmically from 27 to 10 where it seems to be flattening out.

Descriptor Emphasis with Strong Latent Constraint
coord = 2.0
desc = 5.0
collapse = 2.0
beta = 0.2
Moderate layout variation, though slightly clustered and not much variation in voxel numbers, minimal
variation in colour. Scaled losses: collapse penalty drops logarithmically from 35 to near 0 (18-0 raw), desc loss is
near flat around 8 (2 raw), coor loss is almost flat around 3.5 (2 raw). Scaled beta kl forms a mountain 
with a steeper left side and a fluctuating surface, rising from 0.015 to 0.037 and then dropping to 10,
total loss drops logarithmically from 35 to 10.

High Beta  
coord = 3.5
desc = 3.5
collapse = 2.0
beta = 0.5
Best variation in layout and so far though still slightly clustered and seems to be always 6 voxels reconstructed, 
and good variation in colour. Scaled losses: collapse penalty drops rapidly from 15 to near 0 (7-0 raw) in 20 epochs,
desc loss is near flat at 6 (1.75 raw), coor loss has a slight drop at the beginning and a very slight downward trend
from 7 to 4 (2-1 raw). Beta scaled kl is almost convex but is lower on the left side, dropping from 0.015 to 0.005 and then 
exponentially rising to 0.045, total loss drops quickly at the beginning from 30 to 15 in around 5 epochs and then stedily
declines to 10.

Coordinate Descriptor Balance with Standard VAE Beta
coord: 4.0
desc: 4.0
collapse: 1.5
beta: 1.0 
moderate variation in layout, but fairly clustered, limited colour variation. Scaled losses: collapse penalty drops
logarithmically from 15 to near 0 (10-0.5 raw), desc loss is flat at 7 (2 raw), coor loss drops slightly
faster near the beginning from 7 to 5 (2-1.75 raw). Beta scaled KL ocilates slightly on its path, but its main trend 
looks like a mountain on the left (peak aligned with the y axis) and a platform extending out for 10 epochs before decending,
overall it drops from 0.05 to 0.005, total loss drops logarithmically from 30 to 12.

High beta seems to produce better layouts, but more clustering, and starting to loose colour variation.
Best beta seems to be around 0.5. 
Coordinate loss stabilises too early, potentially difficulty learning beyond clusters.
Descriptor loss is mostly flat, potentially not learning from spatial relationships (how descriptor loss is calculated).
Collapse penalty always drops rapidly, easy to minimise, potentially overpowering spatial learning. 
Beta KL has strong correlation with loss trends, indicating latent space is influencing learning.


* Further exploration not using collapse (set to 0 in code), thinking is that coordinate matching loss (entropy) should take care of it
Potentially the collapse loss is causing clustering:

High coordinate
coord = 5.5
desc = 2.5
beta = 0.2
Points are slightly clustered, some but limited variation in layout and colour. Scaled losses: desc loss is flat at 4 (2 raw), Coor loss
drops smoothly and seems to be slowly stabilising from 18 to 7.5 (3.4-1.4 raw). Beta scaled Kl climbs continually with flucuations from near 0 to 0.08,
total loss drops smoothly from 22 to 12.

Descriptor Emphasis, Moderate Beta
coord = 3.5
desc = 5.0
beta = 0.25
Fairly static in voxel count, location variation and colour. Scaled losses: coord loss drops from 5 to 4
in 5 epochs then flatlines until epoch 30 when it drops slightly more to 3 (1.5-0.8 raw), desc loss is fairly flat
at 8 (1.7 raw). Beta scaled kl drops logarithmically with fluctuations from 0.008 to 0.002 and stabilises, total 
loss drops from 13 to 11. 

max coordinate
coord = 6.0
desc = 3.0
beta = 0.3
Fairly static locations, very slight variation in voxel numbers, no colour variation. Scaled losses: Desc loss is flat at 5 (1.7 raw), 
coor loss drops logarithmically from 11 to 5 (2-0.8 raw). Beta scaled KL climbs quickly from 0.025 and spikes at 0.38 before dropping 
quickly to 0.005 and then slowly climbing to 0.013, total loss drops logarithmically from 17 to 10 and seems to be stabilising.

Descriptor Coordinate Balance
coord = 4.0
desc = 4.0
beta = 0.35
Some variation in voxel numbers that seems to follow the original, locations seem a little fixed, no colour variation. Scaled losses: Coor
loss drops logarithmically from 7 to 4 (1.75 to 1 raw), desc loss is almost flat with a slight drop from 1.75 to 1.6. Beta scaled KL slowly climbs
from near 0 to 0.025 for the first 35 epochs before climbing faster to 0.15, total loss drops smoothly from 14 to 10.

Mid Beta with Coordinated Descriptor Boost
coord = 5.0
desc = 5.0
beta = 0.5
Has some variation in voxel numbers, not a lot of variation in location, or colour. Scaled losses: desc loss drops very slowly from 9 to 7.5 (1.75-1.5 raw),
coor loss drops logarithmically from 1.75 and stabilises at 1.0.

When coordinate lambda is high, coordinate loss drops effectively, however it also results in limited spatial or colour variation. 
Requires sufficient descriptor balance. 
Descriptor focused models stabilise descriptor loss, but do not seemt to improve spatial variation.
Strong descriptor lambda without enough spatial guidance seems to lead to descriptor matching without fully aiming for a specific voxel or spatial position.
When coord and desc lambdas are equal, voxel numbers show improvement and better alignment with original.
Beta between 0.2 and 0.5 seem to produce more structured patterns with meaningful changes in KL. 
Coor loss tends to be higher than desc loss. Desc loss seems to remain flat most of the time.
Without collapse penalty, spaitial allignment is being driven by the coordinate matching loss, and this seems to be effective, 
however clustering persists. It seems that a small clustering penalty may have had some benefits to avoid stacking of voxels. 

* Full dataset run

coord = 4.5
desc = 4.0
collapse = 1.0  Reintroduced at a low value to prevent clustering
beta = 0.35
lr = 5e-5  Introduced a scheduler 

Seems there is extremally limited number of voxel and layout variation, particularly with voxel numbers, they hover around 2 or 3 
regardless of the original, colours vary a little.
The model was terminated at epoch 271 via early stop and achieve the following on the final epoch:
Test metrics (averages):
	Recon loss =   3.9585
	KL div =   3.0800
	Accuracy =  48.49%
	F1 score (weighted average) = 0.5080
	Coordinate Euclidean distance: 0.8046 
The scheduler managed to reduce the learning rate to 5.000000000000002e-09

the model maintained relatively stable coordinate and descriptor losses after initial drops, but there was minimal variation in voxel counts and layout. 
Beta scaled KL increased steadily as the learning rate reduced, but the overall reconstruction loss flattened, indicating model might be stuck in a local minimum.


Scaled descriptor loss ended at a factor of two times the coordinate loss and descriptor loss is based on nearest neighbour, so it may be better to reduce descriptor 
loss in relation to coordinate loss.
Though the model may be cheating the coordinate loss somehow.
If the model finds some stable position and accepts the loss, it may be causing the stagnant layouts. 
The descriptor loss should be giving the model a kick because so long as the voxels don't collapse into the same space, 
descriptors are based on nearest neighbour, which should help draw them to their correct locations. 
Earlier runs showed static layouts in both higher descriptor and higher coordinate loss, making it difficult to pinpoint
which way to go. 
Runs with moderate to high coordinate scaling combined with mid to high beta values have shown the most spatial variation.
High descriptor scaling hasn't provided much layout improvement and tends to plateau quickly.
If coordinates are already clustered or incorrect, the model might be “satisfied” with incorrect nearest neighbors, reinforcing the wrong layout.
Though due to past performance, it seems coordinate loss should be higher than descriptor loss.

* Full dataset higher coor, low collapse
epochs = 400

Coor = 5.0 – prioritise spatial alignment early
Desc = 3.5 – keeps descriptor learning active but not dominant, letting coordinates settle first
Collapse = 0.5 – enough to prevent collapse without pulling voxels too close
Beta = 0.4 – strong enough to ensure latent space is meaningful, but not so high it dominates reconstruction loss
Learning rate: 1e-3 – higher for more aggressive early learning and escape potential local minima
Scheduler factor = 0.2 
Scheduler patience = 10
Early stopping patience = 20

Limited variation in location, only 2 or 3 voxels were reconstructed, some slight variation in colour.


** Realised why the descriptor loss may be mostly flat... 
I'm not passing any coordinate information back to the descriptors in the model, I just concatenate the tensors
rather than multiplying or adding. Assume that descriptors certainly need coordinate information as their loss 
is based on location. Also assume that coordinates would benefit from descriptor value information as (in theory 
and at a basic level), components lower down are more likely to be wheels, where as higher up are more likely to 
be sensors (though this is an example and assumption about the robots, due to them being evolved, this may not 
be the case, though I would assume some descriptor information would be beneficial to components).

Added another 1Dconv layer to the encoder for the descriptor mlp to make it the same size as the coordinate mlp output.
Replace the concatentation with additive combination and a LeakyReLU activation. Changed the Linear steps in the fc (refining sequential) to be smaller.
Added another intermediary layer in the decoder fc (upsampling) before split to mirror. 

Quick exploratory (50 epochs, patience 10, no scheduler, 1e-3 lr, collapse=0.5) 
High Coordinate:
coord = 5.0, desc = 2.0, beta = 0.2
OK colour matching, lmited layout variation and voxel count, only 2-3 voxels reconstructed. 
Scaled losses: collapse penalty is near 0 constantly (raw fluctuates, but still very low), coor loss drops from 5 to 0.5 where it 
seems to stabilise (1-0.2 raw), desc loss drops slowly from 3.5 to 2.5 (1.75-1.25 raw). Beta scaled 
kl is fairly flat at 0.2, total loss drops from 8 to 3 logarithmically.

Descriptor:
coord = 2.5, desc = 5.0, beta = 0.3
Some spatial layout variation better voxel number matching. Scaled losses: collapse penalty spikes to 0.14, but mostly remains near 0
(0.25 raw), Desc loss gradually and continually decreases from 8 to 6 (1.5-1.1 raw), coor loss also drops continually from 2 to 0.2 (0.8-0.1 raw).
Beta scaled KL climbs slightly from 0 to 0.5 where it remains flat, total loss gradually decreases from 9 to 6.

Balanced Scaling:
coord = 4.0, desc = 4.0, beta = 0.25
Limited to no spatial and colour variation, no voxel number variation. Scaled losses: collapse penalty pulses with gradually reducing peaks 
(0.08, 0.05, 0.03 which is 0.2, 0.1, 0.8 raw) and seems to stabilise near 0. Desc loss continually decreases from 6.5 to 5 (1.7-1.2 raw), 
coor loss decreases slower near the beginning from 3-2.5 then faster around epoch 10-20 where it drops to 0.5 where it seems to remain flat raw
drops from 0.8 to 0.1. Beta scaled kl seems to remain fairly flat around 0.7, total loss continually decreases but looks like it starts to 
stabilise near the end dropping from 10-5. 

Increased Beta:
coord = 3.5, desc = 3.5, beta = 0.4
Limited spatial and colour variation, no voxel number variation. Scaled losses: collapse remains flat near 0, but raw fluctuates slightly near 
the beginning between near 0 and 0.5, desc loss drops continually from 6-4 (1.75-1.25 raw), coor loss looks like a seat where it is fairly flat
at 3 (0.8 raw) until epoch 10 when it drops to around 0.5 (0.25 raw) around epoch 25. Beta scaled kl climbs slightly but is mostly flat at 0.5,
total loss looks like a seat similar to coor loss, dropping from 9-4.5

Moderate Coordinate, Higher Beta:
coord = 3.0, desc = 2.0, beta = 0.5
Fixed, no variation at all, only 2 blue voxels. Scaled losses: collapse penalty drops from 1 to near 0, where it is flat, desc loss is flat at 2.9 (1.4 raw),
coor loss is seat like flat for 10 epochs at 2.5 where it begins to drop logarithmically down to 0.6 (0.9 - 0.2 raw). Beta scaled kl is flat near 0, total loss
is seat like for 10 epochs at 5.5 and drops to 3.5. 

Note beta values may not be overly accurate due to plotting learning rate which has made the beta kl scale difficult to read. Will change for next run.

Descript emphasis performed the best. 
High desc seems to encourage better voxel spatial awareness. Beta=0.3 had the highest kl div (more active latent space)
Higher beta cause little variation, lower minimsed latent space usage even though reconstruction improved.
collapse drops quickly and stabilises near 0, could mean that the penalty is effective, or it could mean the model is cheating 
and then not exploring - worth reducing slightly.
Higher coor seems to produce minimal variation, seems that descriptor must learn first.
Desc should probably be higher than coor.
Also noticed that the transformed original nolonger seems to match the original.

* Testing extremes

High Coordinate Priority Extreme
coord = 6.0, desc = 1.0, collapse = 0.3, beta = 0.3

High Descriptor Priority Extreme
coord = 1.0, desc = 6.0, collapse = 0.3, beta = 0.3

Low Collapse Penalty
coord = 4.0, desc = 4.0, collapse = 0.1, beta = 0.3

High Collapse Penalty
coord = 3.0, desc = 3.0, collapse = 1.0, beta = 0.3

Balanced but Descriptor First Assumption
coord = 3.0, desc = 5.0, collapse = 0.3, beta = 0.3

Minimal variation all round.
Latent space regulation in comparison to total loss seems better.
Collapse penalty is minimal, potentially leading to voxel sparsity.
When coor > 5 = limited descriptor variation.
Desc > 5 = better descriptor accuracy but worse spatial alignment and increased KL.
Balanced losses seems to produce the most stable training but not produce best latent space diversity.
Potentially the coordinate loss needs to be even higher to avoid local minima. 
Suspect with scaled descriptor loss being higher than scaled coord loss, it is falling into local minima.

* Exploring higher coordinate loss scaling

High Coordinate Emphasis
lambda_coord = 6.0, lambda_desc = 2.0, lambda_collapse = 0.5, beta = 0.3

Moderate Coordinate with Low Descriptor
lambda_coord = 4.0, lambda_desc = 1.0, lambda_collapse = 0.3, beta = 0.3

Balanced with Higher Coordinate Bias
lambda_coord = 5.0, lambda_desc = 3.0, lambda_collapse = 0.5, beta = 0.3

High Coordinate, High Collapse Penalty
lambda_coord = 6.0, lambda_desc = 2.0, lambda_collapse = 1.0, beta = 0.3

Low Descriptor, High Collapse
lambda_coord = 4.5, lambda_desc = 1.5, lambda_collapse = 1.0, beta = 0.3

Descriptor loss seems to remain flat, coordinate loss seems to drop below descriptor loss 
and then start to stabilise, at the point coordinate loss crosses descriptor loss, KL div starts to increase.
It seems desc scaling may be too high, and coor scaling may be too low. Collapse penalty may be counterproductive. 
Beta scaling at 0.3 seems stable.

* High coor low desc

High Coordinate Low Descriptor
coord = 6.0, desc = 0.5, collapse = 0.3, beta = 0.4

Moderate Coordinate, Very Low Descriptor
coord = 5.0, desc = 0.2, collapse = 0.3, beta = 0.35

Balanced Coordinate and Collapse, Low Descriptor
coord = 4.0, desc = 0.4, collapse = 0.5, beta = 0.4

High Coordinate and Collapse, Minimal Descriptor
coord = 7.0, desc = 0.1, collapse = 0.5, beta = 0.35

Extreme Coordinate Weighting
coord = 8.0, desc = 0.3, collapse = 0.3, beta = 0.4

All visualisations show no variation what so every, no location movement, no voxel movement, no colour movement.
Potential that cord loss is pushing model to the extreme (doubtful, but worth trial experiments).

* High desc low coor

1
coord = 1.0, desc = 6.0, collapse = 0.3, beta = 0.3

2
coord = 1.5, desc = 5.0, collapse = 0.3, beta = 0.3

3
coord = 2.0, desc = 4.0, collapse = 0.5, beta = 0.3

4
coord = 2.5, desc = 3.5, collapse = 0.5, beta = 0.3

5
coord = 3.0, desc = 3.0, collapse = 0.3, beta = 0.3

A lot more variation in layout, moderate variation in voxel numbers (though tends to be around 3 or 4), and some variation in colour.
Descriptor loss still seems to be struggling to learn. 

** Added a cross attention layer to allow the coordinate representations to selectively attend to descriptor information, 
so coordinates can learn to focus on the most relevant descriptor features.

Full range grid search setup:
lambda_coord = [2.0, 4.0, 6.0]
lambda_desc = [0.5, 1.0]
lambda_collapse = [0.1, 0.5]
betas = [0.3, 0.5, 0.7]

Still extreamly static layouts. 

** Trying flipping using descriptor as query, and coordinates as keys and values rather than the other way around.

Full range grid search setup:
lambda_coord = [2.0, 4.0, 6.0]
lambda_desc = [0.5, 1.0]
lambda_collapse = [0.1, 0.5]
betas = [0.3, 0.5, 0.7]

Slight improvement in layout variation but neglegable, still extreamly static.
Suspect problem with the losses.

** Found problem with overlap penalty where it could be calculating negative losses, allowing the model to cheat and find static layouts.
However this is unlikely to be affecting desc loss.
Removed nan replacement as no longer needed (from masking padded voxels).

Used gradients to inspect losses, it looks like much higher lambda values are required.
Removed attention and went back to additive combination of pooled layers.

Tried lots of quick runs, and modifications to losses and penalties, however seemed to be making the model worse.
Should have been utilsing branches more to revert back.