* When setup grid search, had 18000 configurations, obviously far too many, cut down to 288.
First experiment run (with full dataset (280375 total samples, 196262 training samples) and 288 configurations), in 12 hours 35 mins, only had got to 3rd epoch and 143872/196262 samples of 1st configuration - excessivly long, need to cut down.


* Scaled back training and grid search configurations, cleaned dataset.
Now training to 30 epochs max with early stop patience 5.
Added a timer to the train/validation loops to track as a metric for evaluation.
Added functionality to summarise data, found duplicate rows and rows with only zero descriptor values (won't add value).
Added function to clean data by removing duplicate rows and rows with only zero values, now dataset has 27017 total samples, splitting into train (18911), val (2702), and test (5404) sets.
Grid search configurations cut down to 4 to hopefully allow for quicker training and analysis early on for architectural modifications:
 - batch_sizes = [64]
 - latent_dims = [2, 4]
 - loss_functions = ["mse", "bce"]
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1]  # Ensures KL divergence doesn't overly regularise latent space, which might hinder reconstruction learning
If training is still taking too long, will need to scale back dataset.
 
Visualisations added, ready to visualise the first training configurations.
Later on may want to interpolate the latent space and visualise (extra functionality required).


* Noticed minority descriptor values (1-4) were being heavily predicted in the reconstruction.
Decided to use logarithmic scaling for class weighting to help give the minority classes a boost so the model pays them more attention.
Retrained base configuration with only this change. 

* Reconstructed outputs are much better than before with the log scaling for class weighting, however training time is still too long for the early training stages.
Reduced train and validation datasets to 20% to reduce training time.
Retrainged the base model using the toy dataset using the log scaling class weights to get a toy set baseline for comparison.
Toy model showed similar performance to the base log scaling model (though slightly reduced as expected due to less data).

* Modified model to convolutional model. 
Original FC layers model (base model) had the following:
Encoder: 4 layers (3 trainable) 
	Flatten, 
	FC,  681984 parameters
	FC,  131328 parameters
	Latent FC branching,  514 parameters for each branch assuming latent dim = 2, so 1028 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  768 parameters assuming latent dim = 2
	FC,  131584 parameters
	FC,  682803 parameters
Total: 6 trainable layers,  1629495 parameters

Convolutional model:
Encoder: 4 layers (3 trainable)
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 3 layers (3 trainable)
	FC,  20736 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters which is fairly close to the original FC model for good comparison

Started training convolutional model.
Base model had a slightly better tradeoff score and loss, whilst the convolution model had a slightly better F1 score,
however all of these are extremely close. 
It should be noted that the best model found in the linear model used a latent dimension of 4, whilst the convolution model used 2. 
The clusters look tighter in the convolution model (good), and there is slight overlapping for both the convolution and linear models (suggesting latent dim is too small).
Due to these being very close, and the clustering being better in the convolution model along with convolution being expected to capture spatial data better than a linear model,
decided to move forward expanding the convolution model.

* Added pooling and upsample layers
Conv_pool model:
Encoder: 6 layers (3 trainable)
	Conv3d,  3584 parameters
	MaxPool3d,  
	Conv3d,  884992 parameters
	MaxPool3d,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (3 trainable)
	FC,  20736 parameters
	Upsample,
	ConvTranspose3d,  884864 parameters
	Upsample,
	ConvTranspose3d,  3457 parameters
Total: 6 trainable layers, 1825285 parameters

The model without pooling showed lower reconstruction loss and higher accuracy (more accurately reconstructs the input).
Clusters in the latent space were better separated using UMAP for the model with pooling (may improve latent representations), 
however they are tigher clusters (PCA) for the model without pooling.
It appears the with pooling looses spatial information.

* Removed rounding and clamping operations from VAE. 
Decoder output is now used directly in the loss functions. 
Values are normalised for BCE, but also for other losses to make them comparable in the grid search.
Retraining model without pooling for comparative results going forward.

* Added convolution skip connection in the hope to capture additional features.
Might be worth testing having just a skip connection in the encoder to prevent overfitting.
Conv_skip model:
Encoder: 5 layers (4 trainable)
	(skip) Conv3d,  88064 parameters
	Conv3d,  3584 parameters
	Conv3d,  884992 parameters
	skip path combined with main path,
	Flatten,
	Latent FC branching,  13826 parameters for each branch assuming latent dim = 2, so 27652 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 4 layers (4 trainable)
	FC,  20736 parameters
	(skip) ConvTranspose3d,  87809 parameters
	ConvTranspose3d,  884864 parameters
	ConvTranspose3d,  3457 parameters
Total: 8 trainable layers, 2001158 parameters

Skip connection model pca clusters overlap slightly, UMAP has better seperation, but clusters still overlap slightly.
The convolution model without the skip connection produced better separation in the latent space.
Fluctuations seen in the reconstruction loss and KL divergence.
Model without skip connection had a constent lower loss, though this may be due to overfitting.
The fluctuations seen with the skip connection may be due to the model trying to predict other values.
Though overall the model without the skip connection performed better.

* Trialing deeper network with LeakyReLU to avoid dying relu (usually better for deeper networks)
Also trialing using conv3d in the decoder to try to refine the input before upsampling.
deep model:
Encoder: 6 layers (5 trainable)
	Conv3d,  1792 parameters
	Conv3d,  221312 parameters
	Conv3d,  884992 parameters
	Conv3d,  1769728 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 5 layers (5 trainable)
	FC,  165888 parameters
	Conv3d,  1769728 parameters
	Conv3d,  884864 parameters
	ConvTranspose3d,  221248 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6142469 parameters

Trialing using the following configurations (4 configurations):
 - batch_sizes = [64]
 - latent_dims = [4, 8]  increasing latent dim
 - loss_functions = ["mse"]  MSE has consistently outperformed BCE.
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1, 0.5]  trialing larger beta value

Demonstrated ability to leverage larger latent space (best model found used latent dim = 8)
Though appears to still be overfitting and biased towards empty space predicting only zeros.
Better reconstruction loss and KL div, though probably because its predicting zeros (simpler outputs).
Potentially may be too heavily regularised by beta 0.1.
Model with best F1 score used beta 0.5, whereas the best performing model used beta 0.1.
Clustering seems more structured latent space distributions, suggesting the architectur does help improve the model.
Increased training configurations:
latent_dims = [4, 8, 16] 
betas = [0.1, 0.5, 1]

* Added batch normalisation in an attempt to stabilise training.

batch_norm model:
Encoder: 10 layers (9 trainable)
	Conv3d,  1792 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  221312 parameters
	BatchNorm3d,  256 parameters
	Conv3d,  884992 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Flatten,
	Latent FC branching,  110594 parameters for each branch assuming latent dim = 2, so 221188 parameters
Sampling: 1 layer (0 trainable)
	Reparameterization step
Decoder: 9 layers (9 trainable)
	FC,  165888 parameters
	BatchNorm1d,  110592 parameters
	Conv3d,  1769728 parameters
	BatchNorm3d,  512 parameters
	Conv3d,  884864 parameters
	BatchNorm3d,  256 parameters
	ConvTranspose3d,  221248 parameters
	BatchNorm3d,  128 parameters
	Conv3d,  1729 parameters
Total: 10 trainable layers, 6255365 parameters

Still overfitting and nothing in the reconstructed output. Looking into sparse matrix representations.

* Setup a dataset class. Within that, obtaining sparse representation of data, padded to max number of voxels in the dataset (8 in this case - shape (8,(x,y,z,one-hot descriptors))
Modified model inspired by pointnet (though simpler to see if that works to start with due to small sparse representation matrix (8x8)
