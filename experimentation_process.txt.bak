When setup grid search, had 18000 configurations, obviously far too many, cut down to 288.
First experiment run (with full dataset (280375 total samples, 196262 training samples) and 288 configurations), in 12 hours 35 mins, only had got to 3rd epoch and 143872/196262 samples of 1st configuration - excessivly long, need to cut down.


Scaled back training and grid search configurations.
Now training to 30 epochs max with early stop patience 5.
Added a timer to the train/validation loops to track as a metric for evaluation.
Added functionality to summarise data, found duplicate rows and rows with only zero descriptor values (won't add value).
Added function to clean data by removing duplicate rows and rows with only zero values, now dataset has 27017 total samples, splitting into train (18911), val (2702), and test (5404) sets.
Grid search configurations cut down to 4 to hopefully allow for quicker training and analysis early on for architectural modifications:
 - batch_sizes = [64]
 - latent_dims = [2, 4]
 - loss_functions = ["mse", "bce"]
 - optimizer = [
        {"type": optim.Adam, "params": {}, "model_name": "adam"}
    ]
 - learning_rates = [1e-3]
 - weight_decay = [0]
 - betas = [0.1]  # Ensures KL divergence doesn't overly regularise latent space, which might hinder reconstruction learning
 
 